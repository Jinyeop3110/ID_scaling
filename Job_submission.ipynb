{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdac45d9-3005-498a-b999-3083ec6061bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9720bd71",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "848b2a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 182759\n"
     ]
    }
   ],
   "source": [
    "    from utils.load_Datasets import load_Datasets\n",
    "    from utils.load_Model import load_Model\n",
    "    from utils.utils import *\n",
    "\n",
    "\n",
    "    dataset_name='pile_uncopyrighted_parquet'\n",
    "    dataset_subset=[0,1]\n",
    "    dataset = load_Datasets(dataset_config_dict={\n",
    "                'name': dataset_name,\n",
    "                'subset': dataset_subset,\n",
    "                'base_directory': None\n",
    "            })\n",
    "    print(f\"Original dataset size: {len(dataset)}\")\n",
    "    def filter_item(index, item, tokenizer, tokens_min_length):\n",
    "        tokens = tokenizer(item['text'], truncation=False, return_tensors='pt')\n",
    "        if tokens['input_ids'].shape[1] >= tokens_min_length:\n",
    "            return index\n",
    "        return None\n",
    "\n",
    "    def filter_dataset_indices(dataset, tokenizer, tokens_min_length=1024, max_workers=4):\n",
    "        valid_indices = []\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(filter_item, i, item, tokenizer, tokens_min_length): i for i, item in enumerate(dataset)}\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                result = future.result()\n",
    "                if result is not None:\n",
    "                    valid_indices.append(result)\n",
    "        valid_indices.sort()\n",
    "        return valid_indices\n",
    "\n",
    "    valid_indices = filter_dataset_indices(dataset, tokenizer, tokens_min_length=tokens_min_length, max_workers=8)\n",
    "    dataset = dataset.select(valid_indices)\n",
    "    print(f\"Filtered dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59310a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '{\\n  \"fpsLimit\": 60,\\n  \"preset\": \"basic\",\\n  \"background\": {\\n    \"color\": \"#0d47a1\",\\n    \"image\": \"\",\\n    \"position\": \"50% 50%\",\\n    \"repeat\": \"no-repeat\",\\n    \"size\": \"cover\"\\n  }\\n}',\n",
       " 'meta': ['Github'],\n",
       " 'id': '0-10'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc9fc7c8-96c5-4a6d-a365-fb7a534c985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240609_111233_olmo-1b_main_pile_uncopyrighted_parquet_0_1\n",
      "Submitted batch job 26219380\n",
      "20240609_111233_olmo-1b_step100000-tokens419B_pile_uncopyrighted_parquet_0_1\n",
      "Submitted batch job 26219381\n",
      "20240609_111233_olmo-1b_step20000-tokens84B_pile_uncopyrighted_parquet_0_1\n",
      "Submitted batch job 26219382\n",
      "20240609_111233_olmo-1b_step350000-tokens1468B_pile_uncopyrighted_parquet_0_1\n",
      "Submitted batch job 26219383\n",
      "20240609_111233_olmo-1b_step400000-tokens1678B_pile_uncopyrighted_parquet_0_1\n",
      "Submitted batch job 26219384\n",
      "20240609_111233_olmo-1b_step450000-tokens1887B_pile_uncopyrighted_parquet_0_1\n",
      "Submitted batch job 26219385\n",
      "20240609_111233_olmo-1b_step50000-tokens210B_pile_uncopyrighted_parquet_0_1\n",
      "Submitted batch job 26219386\n",
      "20240609_111233_olmo-1b_step500000-tokens2097B_pile_uncopyrighted_parquet_0_1\n",
      "Submitted batch job 26219387\n",
      "20240609_111233_olmo-1b_step550000-tokens2307B_pile_uncopyrighted_parquet_0_1\n",
      "Submitted batch job 26219388\n",
      "20240609_111234_olmo-1b_step600000-tokens2517B_pile_uncopyrighted_parquet_0_1\n",
      "Submitted batch job 26219389\n",
      "20240609_111234_olmo-1b_step650000-tokens2726B_pile_uncopyrighted_parquet_0_1\n",
      "Submitted batch job 26219390\n",
      "20240609_111234_olmo-1b_step700000-tokens2936B_pile_uncopyrighted_parquet_0_1\n",
      "Submitted batch job 26219391\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the hyperparameters in Python\n",
    "model_name = \"olmo-1b\"\n",
    "dataset_name = 'pile_uncopyrighted_parquet' #\"openwebtext_parquet\"\n",
    "dataset_subset = [0,1]  # Example list of integers\n",
    "tokens_min_length = 1024\n",
    "\n",
    "model_checkpoints = [\n",
    "    \"main\", \n",
    "    \"step100000-tokens419B\", \n",
    "    \"step20000-tokens84B\", \n",
    "    \"step350000-tokens1468B\", \n",
    "    \"step400000-tokens1678B\", \n",
    "    \"step450000-tokens1887B\", \n",
    "    \"step50000-tokens210B\", \n",
    "    \"step500000-tokens2097B\", \n",
    "    \"step550000-tokens2307B\", \n",
    "    \"step600000-tokens2517B\", \n",
    "    \"step650000-tokens2726B\", \n",
    "    \"step700000-tokens2936B\"\n",
    "]\n",
    "#model_checkpoints =model_checkpoints[:1] \n",
    "# Convert dataset_subset list to a string format\n",
    "dataset_subset_str = ' '.join(map(str, dataset_subset))\n",
    "dataset_subset_str_bar = '_'.join(map(str, dataset_subset))\n",
    "\n",
    "\n",
    "# Function to create and submit a job\n",
    "def submit_job(model_checkpoint):\n",
    "    # Generate current timestamp\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Create dynamic session_name\n",
    "    session_name = f\"{current_time}_{model_name}_{model_checkpoint}_{dataset_name}_{dataset_subset_str_bar}\"\n",
    "    print(session_name)\n",
    "    sbatch_script = f\"\"\"#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name={model_checkpoint}\n",
    "#SBATCH --time=72:10:00\n",
    "#SBATCH --output=LOG/%x-%j.out\n",
    "#SBATCH --error=LOG/%x-%j.err\n",
    "\n",
    "#SBATCH -N 1\n",
    "#SBATCH -c 20\n",
    "#SBATCH --gres=gpu:volta:1\n",
    "#SBATCH --mem-per-cpu=15G\n",
    "\n",
    "echo \"Model checkpoint: {model_checkpoint}\"\n",
    "echo \"CPUs per task: $SLURM_CPUS_PER_TASK\"\n",
    "module load anaconda/2023a-pytorch\n",
    "\n",
    "\n",
    "python Cacheing_OLMo_1B.py --session_name {session_name} --model_name {model_name} --model_checkpoint {model_checkpoint} --dataset_name {dataset_name} --dataset_subset {dataset_subset_str}  --tokens_min_length {tokens_min_length}\n",
    "\"\"\"\n",
    "\n",
    "    script_filename = f\"LLSubScripts/submit_job_{model_checkpoint}.sbatch\"\n",
    "    \n",
    "    with open(script_filename, \"w\") as f:\n",
    "        f.write(sbatch_script)\n",
    "\n",
    "    # Submit the job\n",
    "    subprocess.run([\"sbatch\", script_filename])\n",
    "\n",
    "# Submit a job for each model checkpoint\n",
    "for checkpoint in model_checkpoints:\n",
    "    submit_job(checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c370da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de70775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af66a1d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Setup for model and tokenizer\u001b[39;00m\n\u001b[1;32m     22\u001b[0m module_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.transformer.blocks.15.attn_norm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.transformer.blocks.15.ff_proj\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m hooks \u001b[38;5;241m=\u001b[39m register_hooks(\u001b[43mmodel\u001b[49m, module_list)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Example text generation\u001b[39;00m\n\u001b[1;32m     26\u001b[0m message \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLanguage modeling is \u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac85fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed513e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb05f6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4711e860-4eda-4c9c-bf89-7818be24eaed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d94933-f682-483c-842a-c593ea6ce8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdfd4e4-fa18-4f14-a881-320c59909dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7282c62-5394-41d2-87d4-d10414e187ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3b38b-9c12-4eff-a216-f60efd605641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26170, 14053,   310,   209, 50277, 50279]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af78b37-ac2a-4efd-9990-1a6a983cfc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['step20000-tokens84B',\n",
       "  'step600000-tokens2517B',\n",
       "  'step50000-tokens210B',\n",
       "  'step350000-tokens1468B',\n",
       "  'step550000-tokens2307B',\n",
       "  'step700000-tokens2936B',\n",
       "  'step450000-tokens1887B',\n",
       "  '.ipynb_checkpoints',\n",
       "  'step400000-tokens1678B',\n",
       "  'step500000-tokens2097B',\n",
       "  '.git',\n",
       "  'step100000-tokens419B',\n",
       "  '.gitattributes',\n",
       "  'step650000-tokens2726B',\n",
       "  'main'],\n",
       " ['step450000-tokens1991B',\n",
       "  'step350000-tokens1548B',\n",
       "  'step250000-tokens1106B',\n",
       "  'step50000-tokens221B',\n",
       "  'step20000-tokens88B',\n",
       "  'step400000-tokens1769B',\n",
       "  'step150000-tokens664B',\n",
       "  'step30000-tokens133B',\n",
       "  'step300000-tokens1327B',\n",
       "  'step550000-tokens2433B',\n",
       "  '.ipynb_checkpoints',\n",
       "  'step10000-tokens44B',\n",
       "  '.git',\n",
       "  'step500000-tokens2212B',\n",
       "  'step200000-tokens885B',\n",
       "  'step40000-tokens177B',\n",
       "  '.gitattributes',\n",
       "  'main',\n",
       "  'step100000-tokens442B'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.expanduser(\"~\")+'/ML/Models/OLMo-1B/'), os.listdir(os.path.expanduser(\"~\")+'/ML/Models/OLMo-7B/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d568f11d-b4bf-4b97-8172-78cb94867048",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_path=os.path.expanduser(\"~\")+'/ML/Models/OLMo-1B/'+'step600000-tokens2517B'\n",
    "config_path=model_path+'/config.json'\n",
    "config=AutoConfig.from_pretrained(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da948c73-0556-4402-9e70-8dad6497bc6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OLMoForCausalLM(\n",
       "  (model): OLMo(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(50304, 2048)\n",
       "      (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "      (ln_f): LayerNorm()\n",
       "      (blocks): ModuleList(\n",
       "        (0-15): 16 x OLMoSequentialBlock(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (act): SwiGLU()\n",
       "          (attn_out): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (ff_out): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (attn_norm): LayerNorm()\n",
       "          (ff_norm): LayerNorm()\n",
       "          (att_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (ff_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model=model.to(torch.device(\"cuda\"))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c82f9c-b07f-4014-a260-477d2aebb482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97452955-7ed2-4f37-b734-d1b67cd38c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Hook on Linear\n",
      "Language modeling is \n",
      "    -   a key component in language understanding \n",
      "    -   often used for question-answering or summarization. \n",
      "    -   For a large amount of text data,\n"
     ]
    }
   ],
   "source": [
    "hook_outputs = {}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    #print(f\"Hook on {module.__class__.__name__}\")\n",
    "    hook_outputs[module] = output\n",
    "    return output\n",
    "\n",
    "# Register hooks on the specified layers\n",
    "def register_hooks(model):\n",
    "    hooks = []\n",
    "    layers = model.model.transformer.blocks\n",
    "    \n",
    "    # Register hooks on the 2nd, 3rd, and last layers\n",
    "    layers_to_hook = [1, 2, len(layers) - 1]\n",
    "    \n",
    "    for layer_idx in layers_to_hook:\n",
    "        layer = layers[layer_idx]\n",
    "        \n",
    "        # Register hooks on the feed-forward network output (ff_out)\n",
    "        hooks.append(layer.ff_out.register_forward_hook(hook_fn))\n",
    "    \n",
    "    return hooks\n",
    "\n",
    "model=olmo\n",
    "\n",
    "hooks = register_hooks(model)\n",
    "\n",
    "message = [\"Language modeling is \"]\n",
    "inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False).to(torch.device(\"cuda\"))\n",
    "with torch.no_grad():\n",
    "    response = model.generate(**inputs, max_new_tokens=40, do_sample=True, top_k=50, top_p=0.95)\n",
    "    print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n",
    "for hook in hooks:\n",
    "    hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a17c2-11ca-4aed-b0a1-e66f3a50eb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OLMoForCausalLM(\n",
       "  (model): OLMo(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(50304, 2048)\n",
       "      (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "      (ln_f): LayerNorm()\n",
       "      (blocks): ModuleList(\n",
       "        (0-15): 16 x OLMoSequentialBlock(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (act): SwiGLU()\n",
       "          (attn_out): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (ff_out): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (attn_norm): LayerNorm()\n",
       "          (ff_norm): LayerNorm()\n",
       "          (att_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (ff_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c65efb-b5ed-4d53-b343-12abe9c6fc87",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(message, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, return_token_type_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 30\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(response, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Remove hooks after use\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:1525\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1517\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1518\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1519\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1520\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1522\u001b[0m     )\n\u001b[1;32m   1524\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1532\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1542\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1543\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1548\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1549\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/generation/utils.py:2622\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2619\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2621\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2622\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2630\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/OLMo/hf_olmo/modeling_olmo.py:79\u001b[0m, in \u001b[0;36mOLMoForCausalLM.forward\u001b[0;34m(self, input_ids, inputs_embeds, attention_mask, attention_bias, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m     76\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     90\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mhidden_states\n",
      "File \u001b[0;32m~/OLMo/olmo/model.py:1190\u001b[0m, in \u001b[0;36mOLMo.forward\u001b[0;34m(self, input_ids, input_embeddings, attention_mask, attention_bias, past_key_values, use_cache, last_logits_only, output_hidden_states)\u001b[0m\n\u001b[1;32m   1185\u001b[0m     x, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation_checkpoint_fn(\n\u001b[1;32m   1186\u001b[0m         block, x, attention_bias\u001b[38;5;241m=\u001b[39mattention_bias, layer_past\u001b[38;5;241m=\u001b[39mlayer_past, use_cache\u001b[38;5;241m=\u001b[39muse_cache\n\u001b[1;32m   1187\u001b[0m     )\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;66;03m# shape: (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m     x, cache \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/OLMo/olmo/model.py:689\u001b[0m, in \u001b[0;36mOLMoSequentialBlock.forward\u001b[0;34m(self, x, attention_bias, layer_past, use_cache)\u001b[0m\n\u001b[1;32m    687\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activation_checkpoint_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_norm, x))\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 689\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclip_qkv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     qkv\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclip_qkv, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mclip_qkv)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1581\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1581\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1584\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[37], line 12\u001b[0m, in \u001b[0;36mhook_fn\u001b[0;34m(module, input, output)\u001b[0m\n\u001b[1;32m      9\u001b[0m     cache[full_name] \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdetach()  \u001b[38;5;66;03m# Keep tensor on CUDA\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Concatenate new output tensor along axis=1 with previous tensors, keep on CUDA\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     cache[full_name] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfull_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "                      \n",
    "cache = {}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    # Retrieve the full name from the context in which the hook was registered\n",
    "    full_name = hook_fn.full_names.get(module, \"Unknown\")  \n",
    "    if full_name not in cache:\n",
    "        cache[full_name] = output.detach()  # Keep tensor on CUDA\n",
    "    else:\n",
    "        # Concatenate new output tensor along axis=1 with previous tensors, keep on CUDA\n",
    "        cache[full_name] = torch.cat((cache[full_name], output.detach()), dim=1)\n",
    "\n",
    "def register_hooks(model, module_list):\n",
    "    hooks = []\n",
    "    hook_fn.full_names = {}  # Dictionary to store full names accessible by module object\n",
    "    for name, module in model.named_modules():\n",
    "        if name in module_list:\n",
    "            hooks.append(module.register_forward_hook(hook_fn))\n",
    "            hook_fn.full_names[module] = name\n",
    "    return hooks\n",
    "\n",
    "# Setup for model and tokenizer\n",
    "model = olmo  # Ensure model is correctly assigned\n",
    "module_list = [\"model.transformer.blocks.15.attn_norm\", \"model.transformer.blocks.15.ff_proj\"]\n",
    "hooks = register_hooks(model, module_list)\n",
    "\n",
    "# Example text generation\n",
    "message = [\"Language modeling is \"]\n",
    "inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "with torch.no_grad():\n",
    "    response = model.generate(**inputs, max_new_tokens=40, do_sample=True, top_k=50, top_p=0.95)\n",
    "    print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n",
    "\n",
    "# Remove hooks after use\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "\n",
    "# Move to CPU after all GPU computations if needed\n",
    "for key in cache:\n",
    "    cache[key] = cache[key].cpu()  # Now move tensors to CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d3399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OLMoForCausalLM(\n",
       "  (model): OLMo(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(50304, 2048)\n",
       "      (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "      (ln_f): LayerNorm()\n",
       "      (blocks): ModuleList(\n",
       "        (0-15): 16 x OLMoSequentialBlock(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (act): SwiGLU()\n",
       "          (attn_out): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (ff_out): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (attn_norm): LayerNorm()\n",
       "          (ff_norm): LayerNorm()\n",
       "          (att_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "          (ff_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac13eaa6-e532-4cd9-a633-57d908610ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor is on CPU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tensor = cache['model.transformer.blocks.15.attn_norm']\n",
    "if tensor.is_cuda:\n",
    "    print(\"Tensor is on CUDA device:\", tensor.get_device())\n",
    "else:\n",
    "    print(\"Tensor is on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d5e1be-f00b-49e8-847d-671cafb96fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
