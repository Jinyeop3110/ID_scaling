{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde5f5f3-bfea-4ec7-b287-254bc6c2191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 28 12:51:50 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:D8:00.0 Off |                  Off |\n",
      "| N/A   27C    P0    24W / 150W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706cfcd-16da-4d81-96ed-339db4724606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d4fbf9d-aa7d-4511-9510-6aad149f4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import transformer_lens\n",
    "\n",
    "import sys\n",
    "from fancy_einsum import einsum\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformers import AutoTokenizer, pipeline, logging, AutoModelForCausalLM, AutoConfig\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from transformer_lens.loading_from_pretrained import *\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import einops  # Make sure einops is imported\n",
    "\n",
    "from neel_plotly import line, imshow, scatter\n",
    "import transformer_lens.patching as patching\n",
    "import circuitsvis as cv\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6473f195-7925-4a8c-b92a-044f6bb94267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_HookedTransformer import load_HookedTransformer, customized_get_pretrained_model_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6d24d-d7ed-413d-b64a-87149f0ebe76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49df2c4e-4d2a-44e1-867d-5c9e9933ce6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426e0f71544c4e2e8c22b7c5a6bb02d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-7b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "#model,tokenizer = load_HookedTransformer('gpt2-small')\n",
    "model,tokenizer = load_HookedTransformer('llama-7b')\n",
    "#model,tokenizer = load_HookedTransformer('gpt2-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df06a7-362b-4601-a7b9-ee9aa78c0fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b70f96b-a2a2-4c5a-a838-c70915ae6fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4376ed0-4217-4de4-ae22-599a8dfb5caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef20a710-9512-48e5-8673-26e706c41cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8523b1-30af-4d83-b62e-1ead79a973a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce104e3-1ae0-458a-b93c-54859f69c14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dee4e4-3630-42e2-908c-c355eadc8cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea120a-7ceb-482a-afa5-3b268dc6a0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "model_type=\"gpt2-large\"\n",
    "\n",
    "model_paths = {\n",
    "        'gpt2-small': \"../Models/gpt2-small\",\n",
    "        'gpt2-medium': \"../Models/gpt2-medium\",\n",
    "        'gpt2-large': \"../Models/gpt2-large\",\n",
    "        'gpt2-xl': \"../Models/gpt2-xl\",\n",
    "    }\n",
    "model_cfg={\n",
    "        'gpt2-small':{\n",
    "        \"d_model\": 768,\n",
    "        \"n_layers\": 12,\n",
    "        \"d_mlp\": 3072,\n",
    "        \"d_head\": 64,\n",
    "        \"n_heads\": 12,\n",
    "        \"n_ctx\": 1024,\n",
    "        \"d_vocab\": 50257,\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "        \"act_fn\": \"gelu_new\",\n",
    "        \"attn_only\": False,\n",
    "        \"final_rms\": False,\n",
    "        \"original_architecture\":\"GPT2LMHeadModel\",\n",
    "        },\n",
    "        'gpt2-medium':{\n",
    "        \"d_model\": 1024,\n",
    "        \"n_layers\": 24,\n",
    "        \"d_mlp\": 4096,\n",
    "        \"d_head\": 64,\n",
    "        \"n_heads\": 16,\n",
    "        \"n_ctx\": 1024,\n",
    "        \"d_vocab\": 50257,\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "        \"act_fn\": \"gelu_new\",\n",
    "        \"attn_only\": False,\n",
    "        \"final_rms\": False,\n",
    "        \"original_architecture\":\"GPT2LMHeadModel\",\n",
    "        },\n",
    "            'gpt2-large':{\n",
    "        \"d_model\": 1280,\n",
    "        \"n_layers\": 36,\n",
    "        \"d_mlp\": 5120,\n",
    "        \"d_head\": 64,\n",
    "        \"n_heads\": 20,\n",
    "        \"n_ctx\": 1024,\n",
    "        \"d_vocab\": 50257,\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "        \"act_fn\": \"gelu_new\",\n",
    "        \"attn_only\": False,\n",
    "        \"final_rms\": False,\n",
    "        \"original_architecture\": \"GPT2LMHeadModel\",\n",
    "    },\n",
    "            'gpt2-xl':{\n",
    "        \"d_model\": 1600,\n",
    "        \"n_layers\": 48,\n",
    "        \"d_mlp\": 6400,\n",
    "        \"d_head\": 64,\n",
    "        \"n_heads\": 25,\n",
    "        \"n_ctx\": 1024,\n",
    "        \"d_vocab\": 50257,\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "        \"act_fn\": \"gelu_new\",\n",
    "        \"attn_only\": False,\n",
    "        \"final_rms\": False,\n",
    "        \"original_architecture\": \"GPT2LMHeadModel\",\n",
    "    },     \n",
    "}\n",
    "if True:\n",
    "    tokenizer_path = model_paths[model_type]\n",
    "    model_path = Path(model_paths.get(model_type))\n",
    "    if not model_path:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    \n",
    "    #hf_config = AutoConfig.from_pretrained(model_path)\n",
    "    #config = AutoConfig.from_pretrained(model_path+'/')\n",
    "    #config['model_type']=model_type\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(model_path)#, low_cpu_mem_usage=True)\n",
    "    #hf_model =GPT2Model.from_pretrained(model_path)#, low_cpu_mem_usage=True)\n",
    "    for param in hf_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    cfg = model_cfg[model_type]\n",
    "    cfg= HookedTransformerConfig.from_dict(cfg)\n",
    "\n",
    "    with open(model_path / 'config.json', 'r') as config_file:\n",
    "        cfg_loaded=json.load(config_file)\n",
    "        \n",
    "    model = HookedTransformer(cfg=cfg, tokenizer=tokenizer, move_to_device=False, default_padding_side=\"right\")\n",
    "\n",
    "    #state_dict = hf_model.state_dict()\n",
    "    state_dict = convert_gpt2_weights(hf_model, cfg)\n",
    "    model.load_and_process_state_dict(\n",
    "                state_dict,\n",
    "                fold_ln=True,\n",
    "                center_writing_weights=True,\n",
    "                center_unembed=True,\n",
    "                fold_value_biases=True,\n",
    "                refactor_factored_attn_matrices=False,\n",
    "    )\n",
    "        \n",
    "    model.move_model_modules_to_device()\n",
    "    model.set_use_attn_result(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18ef0a-714e-4687-8310-756a8e498fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f96118-eb18-428b-be21-f090689dbf71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234f1453-4a46-4ec3-a1f5-053ad043c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    model_cfg={\n",
    "        'gpt2-small':{\n",
    "        \"d_model\": 768,\n",
    "        \"n_layers\": 12,\n",
    "        \"d_mlp\": 3072,\n",
    "        \"d_head\": 64,\n",
    "        \"n_heads\": 12,\n",
    "        \"n_ctx\": 1024,\n",
    "        \"d_vocab\": 50257,\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "        \"act_fn\": \"gelu_new\",\n",
    "        \"attn_only\": False,\n",
    "        \"final_rms\": False,\n",
    "        \"original_architecture\":\"GPT2LMHeadModel\",\n",
    "        },\n",
    "        'gpt2-medium':{\n",
    "        \"d_model\": 1024,\n",
    "        \"n_layers\": 24,\n",
    "        \"d_mlp\": 4096,\n",
    "        \"d_head\": 64,\n",
    "        \"n_heads\": 16,\n",
    "        \"n_ctx\": 1024,\n",
    "        \"d_vocab\": 50257,\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "        \"act_fn\": \"gelu_new\",\n",
    "        \"attn_only\": False,\n",
    "        \"final_rms\": False,\n",
    "        \"original_architecture\":\"GPT2LMHeadModel\",\n",
    "        },\n",
    "            'gpt2-large':{\n",
    "        \"d_model\": 1280,\n",
    "        \"n_layers\": 36,\n",
    "        \"d_mlp\": 5120,\n",
    "        \"d_head\": 64,\n",
    "        \"n_heads\": 20,\n",
    "        \"n_ctx\": 1024,\n",
    "        \"d_vocab\": 50257,\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "        \"act_fn\": \"gelu_new\",\n",
    "        \"attn_only\": False,\n",
    "        \"final_rms\": False,\n",
    "        \"original_architecture\": \"GPT2LMHeadModel\",\n",
    "    },\n",
    "            'gpt2-xl':{\n",
    "        \"d_model\": 1600,\n",
    "        \"n_layers\": 48,\n",
    "        \"d_mlp\": 6400,\n",
    "        \"d_head\": 64,\n",
    "        \"n_heads\": 25,\n",
    "        \"n_ctx\": 1024,\n",
    "        \"d_vocab\": 50257,\n",
    "        \"tokenizer_name\": \"gpt2\",\n",
    "        \"act_fn\": \"gelu_new\",\n",
    "        \"attn_only\": False,\n",
    "        \"final_rms\": False,\n",
    "        \"original_architecture\": \"GPT2LMHeadModel\",\n",
    "    },     \n",
    "    }\n",
    "    \n",
    "    cfg = model_cfg[model_type]\n",
    "    cfg= HookedTransformerConfig.from_dict(cfg)\n",
    "\n",
    "    with open(model_path / 'config.json', 'r') as config_file:\n",
    "        cfg_loaded=json.load(config_file)\n",
    "        \n",
    "    model = HookedTransformer(cfg=cfg, tokenizer=tokenizer, move_to_device=False, default_padding_side=\"right\")\n",
    "\n",
    "    #state_dict = hf_model.state_dict()\n",
    "    state_dict = convert_gpt2_weights(hf_model, cfg)\n",
    "    model.load_and_process_state_dict(\n",
    "                state_dict,\n",
    "                fold_ln=True,\n",
    "                center_writing_weights=True,\n",
    "                center_unembed=True,\n",
    "                fold_value_biases=True,\n",
    "                refactor_factored_attn_matrices=False,\n",
    "    )\n",
    "        \n",
    "    model.move_model_modules_to_device()\n",
    "    model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251cb25b-5d27-4f79-8fa7-684f0d61a1cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f22404-998f-4596-bf23-34a9cc35d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb4eb1-668d-4ca9-9445-9f50bb497fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean string 0 <s> When John and Mary went to the shops, John gave the bag to\n",
      "Corrupted string 0 <s> When John and Mary went to the shops, Mary gave the bag to\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Input string:  Mary is not a single token!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClean string 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mto_string(clean_tokens[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrupted string 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mto_string(corrupted_tokens[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m---> 12\u001b[0m answer_token_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[model\u001b[38;5;241m.\u001b[39mto_single_token(answers[i][j]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(answers))], device\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer token indices\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer_token_indices)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClean string 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mto_string(clean_tokens[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrupted string 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mto_string(corrupted_tokens[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m---> 12\u001b[0m answer_token_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[model\u001b[38;5;241m.\u001b[39mto_single_token(answers[i][j]) \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(answers))], device\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer token indices\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer_token_indices)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClean string 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mto_string(clean_tokens[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrupted string 0\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mto_string(corrupted_tokens[\u001b[38;5;241m0\u001b[39m]))\n\u001b[0;32m---> 12\u001b[0m answer_token_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_single_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43manswers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(answers))], device\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer token indices\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer_token_indices)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformer_lens/HookedTransformer.py:908\u001b[0m, in \u001b[0;36mHookedTransformer.to_single_token\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    906\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tokens(string, prepend_bos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m    907\u001b[0m \u001b[38;5;66;03m# If token shape is non-empty, raise error\u001b[39;00m\n\u001b[0;32m--> 908\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput string: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstring\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a single token!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mAssertionError\u001b[0m: Input string:  Mary is not a single token!"
     ]
    }
   ],
   "source": [
    "prompts = ['When John and Mary went to the shops, John gave the bag to', 'When John and Mary went to the shops, Mary gave the bag to', 'When Tom and James went to the park, James gave the ball to', 'When Tom and James went to the park, Tom gave the ball to', 'When Dan and Sid went to the shops, Sid gave an apple to', 'When Dan and Sid went to the shops, Dan gave an apple to', 'After Martin and Amy went to the park, Amy gave a drink to', 'After Martin and Amy went to the park, Martin gave a drink to']\n",
    "answers = [(' Mary', ' John'), (' John', ' Mary'), (' Tom', ' James'), (' James', ' Tom'), (' Dan', ' Sid'), (' Sid', ' Dan'), (' Martin', ' Amy'), (' Amy', ' Martin')]\n",
    "\n",
    "clean_tokens = model.to_tokens(prompts)\n",
    "# Swap each adjacent pair, with a hacky list comprehension\n",
    "corrupted_tokens = clean_tokens[\n",
    "    [(i+1 if i%2==0 else i-1) for i in range(len(clean_tokens)) ]\n",
    "    ]\n",
    "print(\"Clean string 0\", model.to_string(clean_tokens[0]))\n",
    "print(\"Corrupted string 0\", model.to_string(corrupted_tokens[0]))\n",
    "\n",
    "answer_token_indices = torch.tensor([[model.to_single_token(answers[i][j]) for j in range(2)] for i in range(len(answers))], device=model.cfg.device)\n",
    "print(\"Answer token indices\", answer_token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "799056ea-f947-4005-9ca8-0d9caa005809",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer_token_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_logit_diff\u001b[39m(logits, answer_token_indices\u001b[38;5;241m=\u001b[39m\u001b[43manswer_token_indices\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# Get final logits only\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'answer_token_indices' is not defined"
     ]
    }
   ],
   "source": [
    "def get_logit_diff(logits, answer_token_indices=answer_token_indices):\n",
    "    if len(logits.shape)==3:\n",
    "        # Get final logits only\n",
    "        logits = logits[:, -1, :]\n",
    "    correct_logits = logits.gather(1, answer_token_indices[:, 0].unsqueeze(1))\n",
    "    incorrect_logits = logits.gather(1, answer_token_indices[:, 1].unsqueeze(1))\n",
    "    return (correct_logits - incorrect_logits).mean()\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "\n",
    "clean_logit_diff = get_logit_diff(clean_logits, answer_token_indices).item()\n",
    "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "corrupted_logit_diff = get_logit_diff(corrupted_logits, answer_token_indices).item()\n",
    "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3e9daf-46e4-44fd-985d-b8dab7606823",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_logits[4,6,10025:10030] , clean_logits[4,13,10025:10030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed341917-4993-4bf2-96c1-4ca5d4d06db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.embed.state_dict().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f9d526-bb89-46a5-b2c4-04781b88536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.ln_final.state_dict().values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f00e12-d940-4d7a-8e9b-c496495f5995",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(model.blocks[0].mlp.state_dict().values()):\n",
    "  print(i,j)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
