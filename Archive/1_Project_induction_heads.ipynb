{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dde5f5f3-bfea-4ec7-b287-254bc6c2191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 20 20:39:51 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:D8:00.0 Off |                  Off |\n",
      "| N/A   25C    P0    24W / 150W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7706cfcd-16da-4d81-96ed-339db4724606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "def clear_gpu_memory(*args):\n",
    "    \"\"\"\n",
    "    Clears specified PyTorch tensors or models from GPU memory, then clears the GPU memory cache.\n",
    "    \n",
    "    Args:\n",
    "    *args: Variable length argument list. Expected to be PyTorch tensors or models.\n",
    "    \"\"\"\n",
    "    # Attempt to delete each passed argument\n",
    "    for arg in args:\n",
    "        # Check if the argument is a tensor and is on GPU\n",
    "        if isinstance(arg, torch.Tensor) and arg.is_cuda:\n",
    "            # Delete the argument to release its GPU memory\n",
    "            del arg\n",
    "        # If it's a model or other structure with parameters\n",
    "        elif hasattr(arg, 'parameters'):\n",
    "            # Delete each parameter to release its GPU memory\n",
    "            for param in arg.parameters():\n",
    "                if param.is_cuda:\n",
    "                    del param\n",
    "    \n",
    "    # Explicitly collect garbage\n",
    "    gc.collect()\n",
    "    \n",
    "    # Empty the CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"GPU memory cache has been emptied.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4fbf9d-aa7d-4511-9510-6aad149f4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import transformer_lens\n",
    "\n",
    "import sys\n",
    "from fancy_einsum import einsum\n",
    "\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformers import AutoTokenizer, pipeline, logging, AutoModelForCausalLM, AutoConfig\n",
    "from transformer_lens import ActivationCache, HookedTransformer\n",
    "from torchtyping import TensorType as TT\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "from datasets import Dataset\n",
    "#imdb = load_dataset(\"parquet\", data_files={'train': 'imdb/plain_text/train-00000-of-00001.parquet', 'test': 'imdb/plain_text/train-00000-of-00001.parquet'})\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "import einops  # Make sure einops is imported\n",
    "\n",
    "from neel_plotly import line, imshow, scatter\n",
    "import transformer_lens.patching as patching\n",
    "# Load CSV file with pandas\n",
    "#df = pd.read_csv('imdb/IMDB Dataset.csv', nrows=15000)\n",
    "# Convert the DataFrame to a Hugging Face Dataset\n",
    "#dataset = Dataset.from_pandas(df)\n",
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3aae7b-3456-4cb3-9271-961c5d15bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_HookededTransformer(model_type):\n",
    "    if model_type=='Llama-7b':\n",
    "        model_name = \"../Models/Llama-2-7b-hf\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        hf_model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
    "\n",
    "        for param in hf_model.parameters():\n",
    "             param.requires_grad = False\n",
    "        # Move the model to evaluation mode to disable dropout, batchnorm etc., which are not needed during inference\n",
    "        hf_model.eval()\n",
    "        model = HookedTransformer.from_pretrained(\"llama-7b\", hf_model=hf_model, tokenizer= tokenizer, device=\"cuda\", dtype=\"float16\")\n",
    "    elif model_type=='Mistral-7b':\n",
    "        model_name = \"../Models/Mistral-7B-v0.1\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        hf_model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)\n",
    "\n",
    "        for param in hf_model.parameters():\n",
    "             param.requires_grad = False\n",
    "        # Move the model to evaluation mode to disable dropout, batchnorm etc., which are not needed during inference\n",
    "        hf_model.eval()\n",
    "        model = HookedTransformer.from_pretrained('Mistral-7b', hf_model=hf_model, tokenizer= tokenizer, device=\"cuda\", dtype=\"float16\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def new_load_HookededTransformer(model_type):\n",
    "    model_paths = {\n",
    "        'llama-7b': \"../Models/Llama-2-7b-hf\",\n",
    "        'mistral-7b': \"../Models/Mistral-7B-v0.1\",\n",
    "        'attn-only-1l': \"../Models/Attn_Only_1L512W_C4_Code\",\n",
    "        'attn-only-2l': \"../Models/Attn_Only_2L512W_C4_Code\",\n",
    "    }\n",
    "\n",
    "    tokenizer_path = \"../Models/gpt-neox-tokenizer-digits\" if \"attn-only\" in model_type else model_paths[model_type]\n",
    "    model_path = model_paths.get(model_type)\n",
    "    if not model_path:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    \n",
    "    #hf_config = AutoConfig.from_pretrained(model_path)\n",
    "    #config = AutoConfig.from_pretrained(model_path+'/')\n",
    "    #config['model_type']=model_type\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True)\n",
    "    #hf_model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True)\n",
    "\n",
    "    for param in hf_model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    hf_model.eval()\n",
    "\n",
    "    model = HookedTransformer.from_pretrained(model_type, hf_model=hf_model, tokenizer=tokenizer, device=\"cuda\", dtype=\"float16\")\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0e2a8d-89c1-4a85-89ef-65e7cd00f930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b211496f67a148dfb2cc2a364046a712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-7b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "model, tokenizer =new_load_HookededTransformer(model_type='llama-7b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a5f06-be0d-4fca-b35b-0a85b833ba7e",
   "metadata": {},
   "source": [
    "# The head detector is from Nill Nanda's :  https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/demos/Head_Detector_Demo.ipynb#scrollTo=5ikyL8-S7u2Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7470cd4c-ae81-4fbd-aa7e-47736945e434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "from typing import cast, Dict, List, Optional, Tuple, Union\n",
    "from typing_extensions import get_args, Literal\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformer_lens import HookedTransformer, ActivationCache\n",
    "# from transformer_lens.utils import is_lower_triangular, is_square\n",
    "\n",
    "HeadName = Literal[\"previous_token_head\", \"duplicate_token_head\", \"induction_head\"]\n",
    "HEAD_NAMES = cast(List[HeadName], get_args(HeadName))\n",
    "ErrorMeasure = Literal[\"abs\", \"mul\"]\n",
    "\n",
    "LayerHeadTuple = Tuple[int, int]\n",
    "LayerToHead = Dict[int, List[int]]\n",
    "\n",
    "INVALID_HEAD_NAME_ERR = (\n",
    "    f\"detection_pattern must be a Tensor or one of head names: {HEAD_NAMES}; got %s\"\n",
    ")\n",
    "\n",
    "SEQ_LEN_ERR = (\n",
    "    \"The sequence must be non-empty and must fit within the model's context window.\"\n",
    ")\n",
    "\n",
    "DET_PAT_NOT_SQUARE_ERR = \"The detection pattern must be a lower triangular matrix of shape (sequence_length, sequence_length); sequence_length=%d; got detection patern of shape %s\"\n",
    "\n",
    "\n",
    "def detect_head(\n",
    "    model: HookedTransformer,\n",
    "    seq: Union[str, List[str]],\n",
    "    detection_pattern: Union[torch.Tensor, HeadName],\n",
    "    heads: Optional[Union[List[LayerHeadTuple], LayerToHead]] = None,\n",
    "    cache: Optional[ActivationCache] = None,\n",
    "    *,\n",
    "    exclude_bos: bool = False,\n",
    "    exclude_current_token: bool = False,\n",
    "    error_measure: ErrorMeasure = \"mul\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Searches the model (or a set of specific heads, for circuit analysis) for a particular type of attention head.\n",
    "    This head is specified by a detection pattern, a (sequence_length, sequence_length) tensor representing the attention pattern we expect that type of attention head to show.\n",
    "    The detection pattern can be also passed not as a tensor, but as a name of one of pre-specified types of attention head (see `HeadName` for available patterns), in which case the tensor is computed within the function itself.\n",
    "\n",
    "    There are two error measures available for quantifying the match between the detection pattern and the actual attention pattern.\n",
    "\n",
    "    1. `\"mul\"` (default) multiplies both tensors element-wise and divides the sum of the result by the sum of the attention pattern.\n",
    "    Typically, the detection pattern should in this case contain only ones and zeros, which allows a straightforward interpretation of the score:\n",
    "    how big fraction of this head's attention is allocated to these specific query-key pairs?\n",
    "    Using values other than 0 or 1 is not prohibited but will raise a warning (which can be disabled, of course).\n",
    "    2. `\"abs\"` calculates the mean element-wise absolute difference between the detection pattern and the actual attention pattern.\n",
    "    The \"raw result\" ranges from 0 to 2 where lower score corresponds to greater accuracy. Subtracting it from 1 maps that range to (-1, 1) interval,\n",
    "    with 1 being perfect match and -1 perfect mismatch.\n",
    "\n",
    "    **Which one should you use?** `\"abs\"` is likely better for quick or exploratory investigations. For precise examinations where you're trying to\n",
    "    reproduce as much functionality as possible or really test your understanding of the attention head, you probably want to switch to `\"abs\"`.\n",
    "\n",
    "    The advantage of `\"abs\"` is that you can make more precise predictions, and have that measured in the score.\n",
    "    You can predict, for instance, 0.2 attention to X, and 0.8 attention to Y, and your score will be better if your prediction is closer.\n",
    "    The \"mul\" metric does not allow this, you'll get the same score if attention is 0.2, 0.8 or 0.5, 0.5 or 0.8, 0.2.\n",
    "\n",
    "    Args:\n",
    "    ----------\n",
    "        model: Model being used.\n",
    "        seq: String or list of strings being fed to the model.\n",
    "        head_name: Name of an existing head in HEAD_NAMES we want to check. Must pass either a head_name or a detection_pattern, but not both!\n",
    "        detection_pattern: (sequence_length, sequence_length) Tensor representing what attention pattern corresponds to the head we're looking for **or** the name of a pre-specified head. Currently available heads are: `[\"previous_token_head\", \"duplicate_token_head\", \"induction_head\"]`.\n",
    "        heads: If specific attention heads is given here, all other heads' score is set to -1. Useful for IOI-style circuit analysis. Heads can be spacified as a list tuples (layer, head) or a dictionary mapping a layer to heads within that layer that we want to analyze.\n",
    "        cache: Include the cache to save time if you want.\n",
    "        exclude_bos: Exclude attention paid to the beginning of sequence token.\n",
    "        exclude_current_token: Exclude attention paid to the current token.\n",
    "        error_measure: `\"mul\"` for using element-wise multiplication (default). `\"abs\"` for using absolute values of element-wise differences as the error measure.\n",
    "\n",
    "    Returns:\n",
    "    ----------\n",
    "    A (n_layers, n_heads) Tensor representing the score for each attention head.\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    .. code-block:: python\n",
    "\n",
    "        >>> from transformer_lens import HookedTransformer,  utils\n",
    "        >>> from transformer_lens.head_detector import detect_head\n",
    "        >>> import plotly.express as px\n",
    "\n",
    "        >>> def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "        >>>     px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "        >>> model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "        >>> sequence = \"This is a test sequence. This is a test sequence.\"\n",
    "\n",
    "        >>> attention_score = detect_head(model, sequence, \"previous_token_head\")\n",
    "        >>> imshow(attention_score, zmin=-1, zmax=1, xaxis=\"Head\", yaxis=\"Layer\", title=\"Previous Head Matches\")\n",
    "    \"\"\"\n",
    "\n",
    "    cfg = model.cfg\n",
    "    tokens = model.to_tokens(seq).to(cfg.device)\n",
    "    seq_len = tokens.shape[-1]\n",
    "    \n",
    "    # Validate error_measure\n",
    "    \n",
    "    assert error_measure in get_args(ErrorMeasure), f\"Invalid {error_measure=}; valid values are {get_args(ErrorMeasure)}\"\n",
    "\n",
    "    # Validate detection pattern if it's a string\n",
    "    if isinstance(detection_pattern, str):\n",
    "        assert detection_pattern in HEAD_NAMES, (\n",
    "            INVALID_HEAD_NAME_ERR % detection_pattern\n",
    "        )\n",
    "        if isinstance(seq, list):\n",
    "            batch_scores = [detect_head(model, seq, detection_pattern) for seq in seq]\n",
    "            return torch.stack(batch_scores).mean(0)\n",
    "        detection_pattern = cast(\n",
    "            torch.Tensor,\n",
    "            eval(f\"get_{detection_pattern}_detection_pattern(tokens.cpu())\"),\n",
    "        ).to(cfg.device)\n",
    "\n",
    "    # if we're using \"mul\", detection_pattern should consist of zeros and ones\n",
    "    if error_measure == \"mul\" and not set(detection_pattern.unique().tolist()).issubset(\n",
    "        {0, 1}\n",
    "    ):\n",
    "        logging.warning(\n",
    "            \"Using detection pattern with values other than 0 or 1 with error_measure 'mul'\"\n",
    "        )\n",
    "\n",
    "    # Validate inputs and detection pattern shape\n",
    "    assert 1 < tokens.shape[-1] < cfg.n_ctx, SEQ_LEN_ERR\n",
    "    assert (\n",
    "        is_lower_triangular(detection_pattern) and seq_len == detection_pattern.shape[0]\n",
    "    ), DET_PAT_NOT_SQUARE_ERR % (seq_len, detection_pattern.shape)\n",
    "\n",
    "    if cache is None:\n",
    "        _, cache = model.run_with_cache(tokens, remove_batch_dim=True)\n",
    "\n",
    "    if heads is None:\n",
    "        layer2heads = {\n",
    "            layer_i: list(range(cfg.n_heads)) for layer_i in range(cfg.n_layers)\n",
    "        }\n",
    "    elif isinstance(heads, list):\n",
    "        layer2heads = defaultdict(list)\n",
    "        for layer, head in heads:\n",
    "            layer2heads[layer].append(head)\n",
    "    else:\n",
    "        layer2heads = heads\n",
    "\n",
    "    matches = -torch.ones(cfg.n_layers, cfg.n_heads)\n",
    "\n",
    "    for layer, layer_heads in layer2heads.items():\n",
    "        # [n_heads q_pos k_pos]\n",
    "        layer_attention_patterns = cache[\"pattern\", layer, \"attn\"]\n",
    "        for head in layer_heads:\n",
    "            head_attention_pattern = layer_attention_patterns[head, :, :]\n",
    "            head_score = compute_head_attention_similarity_score(\n",
    "                head_attention_pattern,\n",
    "                detection_pattern=detection_pattern,\n",
    "                exclude_bos=exclude_bos,\n",
    "                exclude_current_token=exclude_current_token,\n",
    "                error_measure=error_measure,\n",
    "            )\n",
    "            matches[layer, head] = head_score\n",
    "    return matches\n",
    "\n",
    "\n",
    "# Previous token head\n",
    "def get_previous_token_head_detection_pattern(\n",
    "    tokens: torch.Tensor,  # [batch (1) x pos]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Outputs a detection score for [previous token heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=0O5VOHe9xeZn8Ertywkh7ioc).\n",
    "\n",
    "    Args:\n",
    "      tokens: Tokens being fed to the model.\n",
    "    \"\"\"\n",
    "    detection_pattern = torch.zeros(tokens.shape[-1], tokens.shape[-1])\n",
    "    # Adds a diagonal of 1's below the main diagonal.\n",
    "    detection_pattern[1:, :-1] = torch.eye(tokens.shape[-1] - 1)\n",
    "    return torch.tril(detection_pattern)\n",
    "\n",
    "\n",
    "# Duplicate token head\n",
    "def get_duplicate_token_head_detection_pattern(\n",
    "    tokens: torch.Tensor,  # [batch (1) x pos]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Outputs a detection score for [duplicate token heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=2UkvedzOnghL5UHUgVhROxeo).\n",
    "\n",
    "    Args:\n",
    "      sequence: String being fed to the model.\n",
    "    \"\"\"\n",
    "    # [pos x pos]\n",
    "    token_pattern = tokens.repeat(tokens.shape[-1], 1).numpy()\n",
    "\n",
    "    # If token_pattern[i][j] matches its transpose, then token j and token i are duplicates.\n",
    "    eq_mask = np.equal(token_pattern, token_pattern.T).astype(int)\n",
    "\n",
    "    np.fill_diagonal(\n",
    "        eq_mask, 0\n",
    "    )  # Current token is always a duplicate of itself. Ignore that.\n",
    "    detection_pattern = eq_mask.astype(int)\n",
    "    return torch.tril(torch.as_tensor(detection_pattern).float())\n",
    "\n",
    "\n",
    "# Induction head\n",
    "def get_induction_head_detection_pattern(\n",
    "    tokens: torch.Tensor,  # [batch (1) x pos]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Outputs a detection score for [induction heads](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=_tFVuP5csv5ORIthmqwj0gSY).\n",
    "\n",
    "    Args:\n",
    "      sequence: String being fed to the model.\n",
    "    \"\"\"\n",
    "    duplicate_pattern = get_duplicate_token_head_detection_pattern(tokens)\n",
    "\n",
    "    # Shift all items one to the right\n",
    "    shifted_tensor = torch.roll(duplicate_pattern, shifts=1, dims=1)\n",
    "\n",
    "    # Replace first column with 0's\n",
    "    # we don't care about bos but shifting to the right moves the last column to the first,\n",
    "    # and the last column might contain non-zero values.\n",
    "    zeros_column = torch.zeros(duplicate_pattern.shape[0], 1)\n",
    "    result_tensor = torch.cat((zeros_column, shifted_tensor[:, 1:]), dim=1)\n",
    "    return torch.tril(result_tensor)\n",
    "\n",
    "\n",
    "def get_supported_heads() -> None:\n",
    "    \"\"\"Returns a list of supported heads.\"\"\"\n",
    "    print(f\"Supported heads: {HEAD_NAMES}\")\n",
    "\n",
    "\n",
    "def compute_head_attention_similarity_score(\n",
    "    attention_pattern: torch.Tensor,  # [q_pos k_pos]\n",
    "    detection_pattern: torch.Tensor,  # [seq_len seq_len] (seq_len == q_pos == k_pos)\n",
    "    *,\n",
    "    exclude_bos: bool,\n",
    "    exclude_current_token: bool,\n",
    "    error_measure: ErrorMeasure,\n",
    ") -> float:\n",
    "    \"\"\"Compute the similarity between `attention_pattern` and `detection_pattern`.\n",
    "\n",
    "    Args:\n",
    "      attention_pattern: Lower triangular matrix (Tensor) representing the attention pattern of a particular attention head.\n",
    "      detection_pattern: Lower triangular matrix (Tensor) representing the attention pattern we are looking for.\n",
    "      exclude_bos: `True` if the beginning-of-sentence (BOS) token should be omitted from comparison. `False` otherwise.\n",
    "      exclude_bcurrent_token: `True` if the current token at each position should be omitted from comparison. `False` otherwise.\n",
    "      error_measure: \"abs\" for using absolute values of element-wise differences as the error measure. \"mul\" for using element-wise multiplication (legacy code).\n",
    "    \"\"\"\n",
    "    assert is_square(\n",
    "        attention_pattern\n",
    "    ), f\"Attention pattern is not square; got shape {attention_pattern.shape}\"\n",
    "\n",
    "    # mul\n",
    "\n",
    "    if error_measure == \"mul\":\n",
    "        if exclude_bos:\n",
    "            attention_pattern[:, 0] = 0\n",
    "        if exclude_current_token:\n",
    "            attention_pattern.fill_diagonal_(0)\n",
    "        score = attention_pattern * detection_pattern\n",
    "        return (score.sum() / attention_pattern.sum()).item()\n",
    "\n",
    "    # abs\n",
    "\n",
    "    abs_diff = (attention_pattern - detection_pattern).abs()\n",
    "    assert (abs_diff - torch.tril(abs_diff).to(abs_diff.device)).sum() == 0\n",
    "\n",
    "    size = len(abs_diff)\n",
    "    if exclude_bos:\n",
    "        abs_diff[:, 0] = 0\n",
    "    if exclude_current_token:\n",
    "        abs_diff.fill_diagonal_(0)\n",
    "\n",
    "    return 1 - round((abs_diff.mean() * size).item(), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454efd04-426b-454f-8e5b-5de255baac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util for plotting head detection scores\n",
    "\n",
    "def plot_head_detection_scores(\n",
    "    scores: torch.Tensor,\n",
    "    zmin: float = -1,\n",
    "    zmax: float = 1,\n",
    "    xaxis: str = \"Head\",\n",
    "    yaxis: str = \"Layer\",\n",
    "    title: str = \"Head Matches\"\n",
    ") -> None:\n",
    "    imshow(scores, zmin=zmin, zmax=zmax, xaxis=xaxis, yaxis=yaxis, title=title)\n",
    "\n",
    "def plot_attn_pattern_from_cache(cache: ActivationCache, layer_i: int):\n",
    "    attention_pattern = cache[\"pattern\", layer_i, \"attn\"].squeeze(0)\n",
    "    attention_pattern = einops.rearrange(attention_pattern, \"heads seq1 seq2 -> seq1 seq2 heads\")\n",
    "    print(f\"Layer {layer_i} Attention Heads:\")\n",
    "    return pysvelte.AttentionMulti(tokens=model.to_str_tokens(prompt), attention=attention_pattern)\n",
    "\n",
    "\n",
    "def is_square(x: torch.Tensor) -> bool:\n",
    "    \"\"\"Checks if `x` is a square matrix.\"\"\"\n",
    "    return x.ndim == 2 and x.shape[0] == x.shape[1]\n",
    "\n",
    "def is_lower_triangular(x: torch.Tensor) -> bool:\n",
    "    \"\"\"Checks if `x` is a lower triangular matrix.\"\"\"\n",
    "    if not is_square(x):\n",
    "        return False\n",
    "    return x.equal(x.tril())\n",
    "device: torch.device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c001a6f6-b05c-471f-a751-a2824e81b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_len = 50\n",
    "random_tokens = torch.randint(1000, 10000, (batch_size, seq_len)).to(model.cfg.device)\n",
    "repeated_tokens = einops.repeat(random_tokens, \"batch seq_len -> batch (2 seq_len)\")\n",
    "repeated_tokens[:, 0] = model.tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50001d7e-dd4c-487c-bb98-0bdd9dcf6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hook copied from transformer lens main demo \n",
    "def induction_score_hook(\n",
    "    pattern: TT[\"batch\", \"head_index\", \"dest_pos\", \"source_pos\"],\n",
    "    hook: HookPoint,\n",
    "):\n",
    "    # We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back\n",
    "    # (This only has entries for tokens with index>=seq_len)\n",
    "    induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1-seq_len)\n",
    "    # Get an average score per head\n",
    "    induction_score = einops.reduce(induction_stripe, \"batch head_index position -> head_index\", \"mean\")\n",
    "    # Store the result.\n",
    "    induction_score_store[hook.layer(), :] = induction_score\n",
    "\n",
    "# We make a boolean filter on activation names, that's true only on attention pattern names.\n",
    "pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9d94854-1475-4cd5-9b75-da80b5190e73",
   "metadata": {},
   "outputs": [
    {
     "ename": "LocalEntryNotFoundError",
     "evalue": "An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/urllib3/connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/urllib3/util/connection.py:95\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m socket\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgetaddrinfo returns an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 85\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 101] Network is unreachable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/urllib3/connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/urllib3/connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1042\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/urllib3/connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/urllib3/connection.py:186\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x7f0b4ed67940>: Failed to establish a new connection: [Errno 101] Network is unreachable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/requests/adapters.py:487\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 487\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[0;32m--> 787\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/urllib3/util/retry.py:592\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[1;32m    594\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /NeelNanda/Attn_Only_1L512W_C4_Code/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f0b4ed67940>: Failed to establish a new connection: [Errno 101] Network is unreachable'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:408\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m hf_raise_for_status(response)\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/requests/sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/requests/sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/utils/_http.py:67\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/state/partition1/llgrid/pkg/anaconda/anaconda3-2023a-pytorch/lib/python3.9/site-packages/requests/adapters.py:520\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectionError\u001b[0m: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /NeelNanda/Attn_Only_1L512W_C4_Code/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f0b4ed67940>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: f4ffdd44-8362-48d7-b60b-e78ce4063158)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m induction_scores_per_layer_head \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m checkpoint_indices:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Load the model from the relevant checkpoint by index\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     model_for_this_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mHookedTransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     tokens_seen_for_this_checkpoint \u001b[38;5;241m=\u001b[39m model_for_this_checkpoint\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mcheckpoint_value\n\u001b[1;32m     17\u001b[0m     tokens_trained_on\u001b[38;5;241m.\u001b[39mappend(tokens_seen_for_this_checkpoint)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformer_lens/HookedTransformer.py:1262\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, n_devices, tokenizer, move_to_device, fold_value_biases, default_prepend_bos, default_padding_side, dtype, **from_pretrained_kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m official_model_name \u001b[38;5;241m=\u001b[39m loading\u001b[38;5;241m.\u001b[39mget_official_model_name(model_name)\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;66;03m# Load the config into an HookedTransformerConfig object. If loading from a\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;66;03m# checkpoint, the config object will contain the information about the\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;66;03m# checkpoint\u001b[39;00m\n\u001b[0;32m-> 1262\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43mloading\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pretrained_model_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mofficial_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfold_ln\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_devices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_devices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault_prepend_bos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_prepend_bos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfrom_pretrained_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mpositional_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshortformer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fold_ln:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformer_lens/loading_from_pretrained.py:1023\u001b[0m, in \u001b[0;36mget_pretrained_model_config\u001b[0;34m(model_name, checkpoint_index, checkpoint_value, fold_ln, device, n_devices, default_prepend_bos, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1017\u001b[0m official_model_name \u001b[38;5;241m=\u001b[39m get_official_model_name(model_name)\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1019\u001b[0m     official_model_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeelNanda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m official_model_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArthurConmy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1021\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m official_model_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaidicoot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1022\u001b[0m ):\n\u001b[0;32m-> 1023\u001b[0m     cfg_dict \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_neel_model_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mofficial_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m official_model_name\u001b[38;5;241m.\u001b[39mstartswith(NEED_REMOTE_CODE_MODELS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m     ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformer_lens/loading_from_pretrained.py:939\u001b[0m, in \u001b[0;36mconvert_neel_model_config\u001b[0;34m(official_model_name, **kwargs)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;124;03mLoads the config for a model trained by me (NeelNanda), converted to a dictionary\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03min the HookedTransformerConfig format.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \n\u001b[1;32m    936\u001b[0m \u001b[38;5;124;03mAutoConfig is not supported, because these models are in the HookedTransformer format, so we directly download and load the json.\u001b[39;00m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    938\u001b[0m official_model_name \u001b[38;5;241m=\u001b[39m get_official_model_name(official_model_name)\n\u001b[0;32m--> 939\u001b[0m cfg_json: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file_from_hf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mofficial_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    942\u001b[0m cfg_arch \u001b[38;5;241m=\u001b[39m cfg_json\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marchitecture\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_old\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m official_model_name \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneel-solu-old\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    944\u001b[0m )\n\u001b[1;32m    945\u001b[0m cfg_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    946\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: cfg_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m: cfg_json[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_architecture\u001b[39m\u001b[38;5;124m\"\u001b[39m: cfg_arch,\n\u001b[1;32m    958\u001b[0m }\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformer_lens/utils.py:57\u001b[0m, in \u001b[0;36mdownload_file_from_hf\u001b[0;34m(repo_name, file_name, subfolder, cache_dir, force_is_torch, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_file_from_hf\u001b[39m(\n\u001b[1;32m     45\u001b[0m     repo_name,\n\u001b[1;32m     46\u001b[0m     file_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     51\u001b[0m ):\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    Helper function to download files from the HuggingFace Hub, from subfolder/file_name in repo_name, saving locally to cache_dir and returning the loaded file (if a json or Torch object) and the file path otherwise.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    If it's a Torch file without the \".pth\" extension, set force_is_torch=True to load it as a Torch object.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mselect_compatible_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_hub_download\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m force_is_torch:\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mload(file_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/huggingface_hub/file_download.py:1371\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m         \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[0;32m-> 1371\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[1;32m   1372\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1373\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1374\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is on.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1375\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhead_call_error\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;66;03m# From now on, etag and commit_hash are not None.\u001b[39;00m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m etag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metag must have been retrieved from server\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "from transformer_lens import evals\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "model_to_scores_per_layer_head = {}\n",
    "model_to_tokens_trained_on = {}\n",
    "tokens_trained_on = []\n",
    "induction_scores_per_layer_head = collections.defaultdict(list)\n",
    "\n",
    "\n",
    "\n",
    "    for index in checkpoint_indices:\n",
    "        # Load the model from the relevant checkpoint by index\n",
    "        model_for_this_checkpoint = HookedTransformer.from_pretrained(model_name, checkpoint_index=index, device=device)\n",
    "\n",
    "        tokens_seen_for_this_checkpoint = model_for_this_checkpoint.cfg.checkpoint_value\n",
    "        tokens_trained_on.append(tokens_seen_for_this_checkpoint)\n",
    "\n",
    "        # induction_score_hook will store results here\n",
    "        induction_score_store = torch.zeros((model_for_this_checkpoint.cfg.n_layers, model_for_this_checkpoint.cfg.n_heads), device=model_for_this_checkpoint.cfg.device)\n",
    "\n",
    "        model_for_this_checkpoint.run_with_hooks(\n",
    "            repeated_tokens, \n",
    "            return_type=None, # For efficiency, we don't need to calculate the logits\n",
    "            fwd_hooks=[(\n",
    "                pattern_hook_names_filter,\n",
    "                induction_score_hook\n",
    "            )]\n",
    "        )\n",
    "\n",
    "        for layer in range(model_for_this_checkpoint.cfg.n_layers):\n",
    "            for head in range(model_for_this_checkpoint.cfg.n_heads):\n",
    "                induction_scores_per_layer_head[str(layer) + ',' + str(head)].append(induction_score_store[layer][head].item())\n",
    "    model_to_scores_per_layer_head[model_name] = induction_scores_per_layer_head\n",
    "    model_to_tokens_trained_on[model_name] = tokens_trained_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e9e75-fcb0-44ee-a4c4-5b2863c0ac6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c0044-9cdd-45be-b530-3496e28ecf3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598a7b0-323c-4429-86ff-5683c2f01bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a13c1-54fb-4b02-837d-0e12bcb87ed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee0a22-91c4-44f9-89f5-f607d08171c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad196b4-5af7-4102-8517-6666dd959a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b20547-47dc-433c-b99c-aaf9d0cf7fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f4ea9-cd7b-4782-a5e7-238a4cb530c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f441e6c3-0d10-4464-9079-8fc5f3a38482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1f2ef-8e13-415c-b55f-843a35109bc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9b75e9-d40b-4636-b5d1-0010a155dfcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e29c9-3585-4997-8f9a-cb4484e596dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13688825-2842-47df-b73d-bf3a77091edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "model, tokenizer = load_HookededTransformer(model_type='Llama-7b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf04108-1fda-4d72-9b3e-6cdf6ac58936",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to \"\n",
    "example_answer = 'Mary'\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93763a08-cc98-40e9-9506-8ea9ec93a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_format = [\n",
    "    #\"When John and Mary went to the shops,{} gave the bag to \",\n",
    "    \"When Tom and James went to the park,{} gave the ball to \",\n",
    "    #\"When Dan and Sid went to the shops,{} gave an apple to \",\n",
    "    \"After Martin and Max went to the park,{} gave a drink to \",\n",
    "]\n",
    "names = [\n",
    "    #(\"Mary\", \"John\"),\n",
    "    (\"Tom\", \"James\"),\n",
    "    #(\"Dan\", \"Sid\"),\n",
    "    (\"Martin\", \"Max\"),\n",
    "]\n",
    "# List of prompts\n",
    "prompts = []\n",
    "# List of answers, in the format (correct, incorrect)\n",
    "answers = []\n",
    "# List of the token (ie an integer) corresponding to each answer, in the format (correct_token, incorrect_token)\n",
    "answer_tokens = []\n",
    "for i in range(len(prompt_format)):\n",
    "    for j in range(2):\n",
    "        answers.append((names[i][j], names[i][1 - j]))\n",
    "        answer_tokens.append(\n",
    "            (\n",
    "                model.to_single_token(answers[-1][0]),\n",
    "                model.to_single_token(answers[-1][1]),\n",
    "            )\n",
    "        )\n",
    "        # Insert the *incorrect* answer to the prompt, making the correct answer the indirect object.\n",
    "        prompts.append(prompt_format[i].format(answers[-1][1]))\n",
    "answer_tokens = torch.tensor(answer_tokens).to(device)\n",
    "print(prompts)\n",
    "print(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f1169-f095-4bb9-ab73-ee9af8d7c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    str_tokens = model.to_str_tokens(prompt)\n",
    "    print(\"Prompt length:\", len(str_tokens))\n",
    "    print(\"Prompt as tokens:\", str_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a06c5a1-f343-4359-90ff-c032858d3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = model.to_tokens(prompts, prepend_bos=True)\n",
    "\n",
    "# Run the model and cache all activations\n",
    "original_logits, cache = model.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ae365-7a5e-4a3e-9690-178a83c61b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tom, James, and average\n",
    "print(original_logits[0,15,4335].item() , original_logits[0,15,5011].item(), torch.mean(original_logits[0,15,:]).item())\n",
    "print(original_logits[1,15,4335].item() , original_logits[1,15,5011].item(), torch.mean(original_logits[1,15,:]).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e31a03-17a0-4948-813d-5eec42a59591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_ave_logit_diff(logits, answer_tokens, per_prompt=False):\n",
    "    # Only the final logits are relevant for the answer\n",
    "    final_logits = logits[:, -1, :]\n",
    "    answer_logits = final_logits.gather(dim=-1, index=answer_tokens)\n",
    "    answer_logit_diff = answer_logits[:, 0] - answer_logits[:, 1]\n",
    "    if per_prompt:\n",
    "        return answer_logit_diff\n",
    "    else:\n",
    "        return answer_logit_diff.mean()\n",
    "\n",
    "\n",
    "print(\n",
    "    \"Per prompt logit difference:\",\n",
    "    logits_to_ave_logit_diff(original_logits, answer_tokens, per_prompt=True)\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .round(decimals=3),\n",
    ")\n",
    "original_average_logit_diff = logits_to_ave_logit_diff(original_logits, answer_tokens)\n",
    "print(\n",
    "    \"Average logit difference:\",\n",
    "    round(logits_to_ave_logit_diff(original_logits, answer_tokens).item(), 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcbaf04-74ae-44e1-ba8e-7cb87d2f0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_residual_directions = model.tokens_to_residual_directions(answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "logit_diff_directions = (\n",
    "    answer_residual_directions[:, 0] - answer_residual_directions[:, 1]\n",
    ")\n",
    "print(\"Logit difference directions shape:\", logit_diff_directions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e045fd-b97c-477e-9ee5-1d06e8082b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream = cache[\"resid_post\", -1]\n",
    "print(\"Final residual stream shape:\", final_residual_stream.shape)\n",
    "final_token_residual_stream = final_residual_stream[:, -1, :]\n",
    "# Apply LayerNorm scaling\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = model.ln_final(\n",
    "    final_token_residual_stream.unsqueeze(1)\n",
    ").squeeze(1)\n",
    "\n",
    "average_logit_diff = einsum(\n",
    "    \"batch d_model, batch d_model -> \",\n",
    "    scaled_final_token_residual_stream,\n",
    "    logit_diff_directions,\n",
    ") / len(prompts) \n",
    "print(\"Calculated average logit diff:\", round(average_logit_diff.item(), 3))\n",
    "print(\"Original logit difference:\", round(original_average_logit_diff.item(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067eb572-3403-491c-b085-be05c370a10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxtyping import Float\n",
    "\n",
    "def residual_stack_to_logit_diff(\n",
    "    residual_stack: Float[torch.Tensor, \"components batch d_model\"],\n",
    "    cache: ActivationCache,\n",
    ") -> float:\n",
    "    scaled_residual_stack = model.ln_final(\n",
    "        residual_stack.unsqueeze(1)).squeeze(1)\n",
    "    return einsum(\n",
    "        \"... batch d_model, batch d_model -> ...\",\n",
    "        scaled_residual_stack,\n",
    "        logit_diff_directions,\n",
    "    ) / len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b1753-1e5b-433c-98eb-055413ee7270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33edaa-bf70-427d-a4d5-8b19f68bedf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_residual, labels = cache.accumulated_resid(\n",
    "    layer=-1, incl_mid=True, pos_slice=-1, return_labels=True\n",
    ")\n",
    "logit_lens_logit_diffs = residual_stack_to_logit_diff(accumulated_residual, cache)\n",
    "line(\n",
    "    logit_lens_logit_diffs,\n",
    "    x=np.arange(model.cfg.n_layers * 2 + 1) / 2,\n",
    "    hover_name=labels,\n",
    "    title=\"Logit Difference From Accumulate Residual Stream\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cab292-1c99-405f-a47a-4146548d7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_layer_residual, labels = cache.decompose_resid(\n",
    "    layer=-1, pos_slice=-1, return_labels=True\n",
    ")\n",
    "per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache)\n",
    "line(per_layer_logit_diffs, hover_name=labels, title=\"Logit Difference From Each Layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc7bb2a-c118-4ab7-86d0-914e96bb008d",
   "metadata": {},
   "source": [
    "# Look into heads level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a94b4d-84e3-4022-89aa-982f52dd70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_patterns(\n",
    "    heads: Union[List[int], int, Float[torch.Tensor, \"heads\"]],\n",
    "    local_cache: ActivationCache,\n",
    "    local_tokens: torch.Tensor,\n",
    "    title: Optional[str] = \"\",\n",
    "    max_width: Optional[int] = 700,\n",
    ") -> str:\n",
    "    # If a single head is given, convert to a list\n",
    "    if isinstance(heads, int):\n",
    "        heads = [heads]\n",
    "\n",
    "    # Create the plotting data\n",
    "    labels: List[str] = []\n",
    "    patterns: List[Float[torch.Tensor, \"dest_pos src_pos\"]] = []\n",
    "\n",
    "    # Assume we have a single batch item\n",
    "    batch_index = 0\n",
    "\n",
    "    for head in heads:\n",
    "        # Set the label\n",
    "        layer = head // model.cfg.n_heads\n",
    "        head_index = head % model.cfg.n_heads\n",
    "        labels.append(f\"L{layer}H{head_index}\")\n",
    "\n",
    "        # Get the attention patterns for the head\n",
    "        # Attention patterns have shape [batch, head_index, query_pos, key_pos]\n",
    "        patterns.append(local_cache[\"attn\", layer][batch_index, head_index])\n",
    "\n",
    "    # Convert the tokens to strings (for the axis labels)\n",
    "    str_tokens = model.to_str_tokens(local_tokens)\n",
    "\n",
    "    # Combine the patterns into a single tensor\n",
    "    patterns: Float[torch.Tensor, \"head_index dest_pos src_pos\"] = torch.stack(\n",
    "        patterns, dim=0\n",
    "    )\n",
    "\n",
    "    # Circuitsvis Plot (note we get the code version so we can concatenate with the title)\n",
    "    display(attention_heads(\n",
    "        attention=patterns, tokens=str_tokens, attention_head_names=labels\n",
    "    ))\n",
    "\n",
    "    # Display the title\n",
    "    title_html = f\"<h2>{title}</h2><br/>\"\n",
    "\n",
    "    # Return the visualisation as raw code\n",
    "    #return f\"<div style='max-width: {str(max_width)}px;'>{title_html + plot}</div>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f504001-73b6-4a05-a075-4917710eaa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_head_residual.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae272b2e-b507-4361-89e2-fac0daa85cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_head_residual, labels = cache.stack_head_results(\n",
    "    layer=-1, pos_slice=-1, return_labels=True\n",
    ")\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)\n",
    "per_head_logit_diffs = einops.rearrange(\n",
    "    per_head_logit_diffs,\n",
    "    \"(layer head_index) -> layer head_index\",\n",
    "    layer=model.cfg.n_layers,\n",
    "    head_index=model.cfg.n_heads,\n",
    ")\n",
    "imshow(\n",
    "    per_head_logit_diffs,\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    "    title=\"Logit Difference From Each Head\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e8fe59-efeb-423e-8ea3-4fe09fcd257f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from circuitsvis.attention import attention_heads\n",
    "from IPython.display import HTML, IFrame\n",
    "\n",
    "\n",
    "top_k = 3\n",
    "\n",
    "top_positive_logit_attr_heads = torch.topk(\n",
    "    per_head_logit_diffs.flatten(), k=top_k\n",
    ").indices\n",
    "\n",
    "positive_html = visualize_attention_patterns(\n",
    "    top_positive_logit_attr_heads,\n",
    "    cache,\n",
    "    tokens[2],\n",
    "    f\"Top {top_k} Positive Logit Attribution Heads\",\n",
    ")\n",
    "\n",
    "top_negative_logit_attr_heads = torch.topk(\n",
    "    -per_head_logit_diffs.flatten(), k=top_k\n",
    ").indices\n",
    "\n",
    "negative_html = visualize_attention_patterns(\n",
    "    top_negative_logit_attr_heads,\n",
    "    cache,\n",
    "    tokens[2],\n",
    "    title=f\"Top {top_k} Negative Logit Attribution Heads\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe93ca6e-35ae-47ff-9506-3a9fa5e37f22",
   "metadata": {},
   "source": [
    "# Activation patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc6256-5253-478f-83c7-3f841a156465",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_prompts = []\n",
    "for i in range(0, len(prompts), 2):\n",
    "    corrupted_prompts.append(prompts[i + 1])\n",
    "    corrupted_prompts.append(prompts[i])\n",
    "corrupted_tokens = model.to_tokens(corrupted_prompts, prepend_bos=True)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(\n",
    "    corrupted_tokens, return_type=\"logits\"\n",
    ")\n",
    "corrupted_average_logit_diff = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)\n",
    "print(\"Corrupted Average Logit Diff\", round(corrupted_average_logit_diff.item(), 2))\n",
    "print(\"Clean Average Logit Diff\", round(original_average_logit_diff.item(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c8bcfc-54da-4bd1-a268-de50239742a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_string(corrupted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c973815-96fe-4c57-9ed4-9cfd8540f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def patch_residual_component(\n",
    "    corrupted_residual_component: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "    hook,\n",
    "    pos,\n",
    "    clean_cache,\n",
    "):\n",
    "    corrupted_residual_component[:, pos, :] = clean_cache[hook.name][:, pos, :]\n",
    "    return corrupted_residual_component\n",
    "\n",
    "\n",
    "def normalize_patched_logit_diff(patched_logit_diff):\n",
    "    # Subtract corrupted logit diff to measure the improvement, divide by the total improvement from clean to corrupted to normalise\n",
    "    # 0 means zero change, negative means actively made worse, 1 means totally recovered clean performance, >1 means actively *improved* on clean performance\n",
    "    return (patched_logit_diff - corrupted_average_logit_diff) / (\n",
    "        original_average_logit_diff - corrupted_average_logit_diff\n",
    "    )\n",
    "\n",
    "\n",
    "patched_residual_stream_diff = torch.zeros(\n",
    "    model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32\n",
    ")\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for position in range(tokens.shape[1]):\n",
    "        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)\n",
    "        patched_logits = model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(utils.get_act_name(\"resid_pre\", layer), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
    "\n",
    "        patched_residual_stream_diff[layer, position] = normalize_patched_logit_diff(\n",
    "            patched_logit_diff\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7bf63-5ec5-45a9-93ff-67a4b47f4b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_position_labels = [\n",
    "    f\"{tok}_{i}\" for i, tok in enumerate(model.to_str_tokens(tokens[0]))\n",
    "]\n",
    "imshow(\n",
    "    patched_residual_stream_diff,\n",
    "    x=prompt_position_labels,\n",
    "    title=\"Logit Difference From Patched Residual Stream\",\n",
    "    labels={\"x\": \"Position\", \"y\": \"Layer\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db2486-6ca2-4232-a1aa-a74ace0603ac",
   "metadata": {},
   "source": [
    "# Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a650eb2-920a-4749-b6e9-eb72663c5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_attn_diff = torch.zeros(\n",
    "    model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32\n",
    ")\n",
    "patched_mlp_diff = torch.zeros(\n",
    "    model.cfg.n_layers, tokens.shape[1], device=device, dtype=torch.float32\n",
    ")\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for position in range(tokens.shape[1]):\n",
    "        hook_fn = partial(patch_residual_component, pos=position, clean_cache=cache)\n",
    "        patched_attn_logits = model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(utils.get_act_name(\"attn_out\", layer), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_attn_logit_diff = logits_to_ave_logit_diff(\n",
    "            patched_attn_logits, answer_tokens\n",
    "        )\n",
    "        patched_mlp_logits = model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(utils.get_act_name(\"mlp_out\", layer), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_mlp_logit_diff = logits_to_ave_logit_diff(\n",
    "            patched_mlp_logits, answer_tokens\n",
    "        )\n",
    "\n",
    "        patched_attn_diff[layer, position] = normalize_patched_logit_diff(\n",
    "            patched_attn_logit_diff\n",
    "        )\n",
    "        patched_mlp_diff[layer, position] = normalize_patched_logit_diff(\n",
    "            patched_mlp_logit_diff\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4b9a2d-7d16-4c98-a949-df02bbd168dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(\n",
    "    patched_attn_diff,\n",
    "    x=prompt_position_labels,\n",
    "    title=\"Logit Difference From Patched Attention Layer\",\n",
    "    labels={\"x\": \"Position\", \"y\": \"Layer\"},\n",
    ")\n",
    "imshow(\n",
    "    patched_mlp_diff,\n",
    "    x=prompt_position_labels,\n",
    "    title=\"Logit Difference From Patched MLP Layer\",\n",
    "    labels={\"x\": \"Position\", \"y\": \"Layer\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab9a81b-afea-4583-9e1f-620f0c6bf97d",
   "metadata": {},
   "source": [
    "# Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783007bc-5109-4424-9da3-94515745d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_head_vector(\n",
    "    corrupted_head_vector: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook,\n",
    "    head_index,\n",
    "    clean_cache,\n",
    "):\n",
    "    corrupted_head_vector[:, :, head_index, :] = clean_cache[hook.name][\n",
    "        :, :, head_index, :\n",
    "    ]\n",
    "    return corrupted_head_vector\n",
    "\n",
    "\n",
    "patched_head_z_diff = torch.zeros(\n",
    "    model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=torch.float32\n",
    ")\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for head_index in range(model.cfg.n_heads):\n",
    "        hook_fn = partial(patch_head_vector, head_index=head_index, clean_cache=cache)\n",
    "        patched_logits = model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(utils.get_act_name(\"z\", layer, \"attn\"), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
    "\n",
    "        patched_head_z_diff[layer, head_index] = normalize_patched_logit_diff(\n",
    "            patched_logit_diff\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a4a21-a50e-41e0-b191-4ef81490fbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6ebdd8-54b2-4dee-a5b2-88f0600673ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache[(utils.get_act_name(\"z\", layer, \"attn\"))].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698517a-3493-4723-896e-4d59d490c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_head_v_diff = torch.zeros(\n",
    "    model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=torch.float32\n",
    ")\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for head_index in range(model.cfg.n_heads):\n",
    "        hook_fn = partial(patch_head_vector, head_index=head_index, clean_cache=cache)\n",
    "        patched_logits = model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(utils.get_act_name(\"v\", layer, \"attn\"), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
    "\n",
    "        patched_head_v_diff[layer, head_index] = normalize_patched_logit_diff(\n",
    "            patched_logit_diff\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d050e-4fa9-4cc7-96a7-2afa9b17eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(\n",
    "    patched_head_v_diff,\n",
    "    title=\"Logit Difference From Patched Head Value\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a63268-dbe7-47cb-8c29-9ef75ca77706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(\n",
    "        y=y,\n",
    "        x=x,\n",
    "        labels={\"x\": xaxis, \"y\": yaxis, \"color\": caxis},\n",
    "        **kwargs,\n",
    "    ).show()\n",
    "    \n",
    "head_labels = [\n",
    "    f\"L{l}H{h}\" for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)\n",
    "]\n",
    "scatter(\n",
    "    x=utils.to_numpy(patched_head_v_diff.flatten()),\n",
    "    y=utils.to_numpy(patched_head_z_diff.flatten()),\n",
    "    xaxis=\"Value Patch\",\n",
    "    yaxis=\"Output Patch\",\n",
    "    caxis=\"Layer\",\n",
    "    hover_name=head_labels,\n",
    "    color=einops.repeat(\n",
    "        np.arange(model.cfg.n_layers), \"layer -> (layer head)\", head=model.cfg.n_heads\n",
    "    ),\n",
    "    range_x=(-0.5, 0.5),\n",
    "    range_y=(-0.5, 0.5),\n",
    "    title=\"Scatter plot of output patching vs value patching\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e2619-cc9e-48fa-803b-b38b81801b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patch_head_pattern(\n",
    "    corrupted_head_pattern: Float[torch.Tensor, \"batch head_index query_pos d_head\"],\n",
    "    hook,\n",
    "    head_index,\n",
    "    clean_cache,\n",
    "):\n",
    "    corrupted_head_pattern[:, head_index, :, :] = clean_cache[hook.name][\n",
    "        :, head_index, :, :\n",
    "    ]\n",
    "    return corrupted_head_pattern\n",
    "\n",
    "\n",
    "patched_head_attn_diff = torch.zeros(\n",
    "    model.cfg.n_layers, model.cfg.n_heads, device=device, dtype=torch.float32\n",
    ")\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for head_index in range(model.cfg.n_heads):\n",
    "        hook_fn = partial(patch_head_pattern, head_index=head_index, clean_cache=cache)\n",
    "        patched_logits = model.run_with_hooks(\n",
    "            corrupted_tokens,\n",
    "            fwd_hooks=[(utils.get_act_name(\"attn\", layer, \"attn\"), hook_fn)],\n",
    "            return_type=\"logits\",\n",
    "        )\n",
    "        patched_logit_diff = logits_to_ave_logit_diff(patched_logits, answer_tokens)\n",
    "\n",
    "        patched_head_attn_diff[layer, head_index] = normalize_patched_logit_diff(\n",
    "            patched_logit_diff\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1408b6e-3286-4b71-8d8c-bf9005c46e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(\n",
    "    patched_head_attn_diff,\n",
    "    title=\"Logit Difference From Patched Head Pattern\",\n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"},\n",
    ")\n",
    "head_labels = [\n",
    "    f\"L{l}H{h}\" for l in range(model.cfg.n_layers) for h in range(model.cfg.n_heads)\n",
    "]\n",
    "scatter(\n",
    "    x=utils.to_numpy(patched_head_attn_diff.flatten()),\n",
    "    y=utils.to_numpy(patched_head_z_diff.flatten()),\n",
    "    hover_name=head_labels,\n",
    "    xaxis=\"Attention Patch\",\n",
    "    yaxis=\"Output Patch\",\n",
    "    title=\"Scatter plot of output patching vs attention patching\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794d13f9-23b6-462d-8d58-682578f4eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc981c98-742d-43b8-bd09-45eb0f0a3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_heads_by_output_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70028e7-23e0-4437-9a83-6fa52b9aa108",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "top_heads_by_output_patch = torch.topk(\n",
    "    patched_head_z_diff.abs().flatten(), k=top_k\n",
    ").indices\n",
    "first_mid_layer = 16\n",
    "first_late_layer = 20\n",
    "early_heads = top_heads_by_output_patch[\n",
    "    top_heads_by_output_patch < model.cfg.n_heads * first_mid_layer\n",
    "]\n",
    "mid_heads = top_heads_by_output_patch[\n",
    "    torch.logical_and(\n",
    "        model.cfg.n_heads * first_mid_layer <= top_heads_by_output_patch,\n",
    "        top_heads_by_output_patch < model.cfg.n_heads * first_late_layer,\n",
    "    )\n",
    "]\n",
    "late_heads = top_heads_by_output_patch[\n",
    "    model.cfg.n_heads * first_late_layer <= top_heads_by_output_patch\n",
    "]\n",
    "\n",
    "early = visualize_attention_patterns(\n",
    "    early_heads, cache, tokens[0], title=f\"Top Early Heads\"\n",
    ")\n",
    "mid = visualize_attention_patterns(\n",
    "    mid_heads, cache, tokens[0], title=f\"Top Middle Heads\"\n",
    ")\n",
    "late = visualize_attention_patterns(\n",
    "    late_heads, cache, tokens[0], title=f\"Top Late Heads\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba76a05-3fa0-43a5-a619-38d53bf83cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
