{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdac45d9-3005-498a-b999-3083ec6061bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9720bd71",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ef29085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5de1220a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load model : gemma-7b from ../Models/Gemma-7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dbe6cbedd314ae4882ba27fcb3e97b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ModelConfig:\n",
    "    def __init__(self, model_name, model_checkpoint=None):\n",
    "        self.model_name = model_name\n",
    "        self.model_checkpoint = model_checkpoint\n",
    "\n",
    "model, tokenizer = load_Model(ModelConfig(\"gemma-7b\",\"main\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96655212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[1, 259]], 'attention_mask': [[1, 1]]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08ec3c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[209]], 'token_type_ids': [[0]], 'attention_mask': [[1]]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a761a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OLMoSequentialBlock(\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       "  (act): SwiGLU()\n",
       "  (attn_out): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "  (ff_out): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "  (rotary_emb): RotaryEmbedding()\n",
       "  (attn_norm): LayerNorm()\n",
       "  (ff_norm): LayerNorm()\n",
       "  (att_proj): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "  (ff_proj): Linear(in_features=4096, out_features=22016, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.transformer.blocks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "848b2a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting job for 20240810_015346_llama2-7b_main_pile_uncopyrighted_parquet_test_0\n",
      "Submitted batch job 26768542\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameters\n",
    "model_name = 'llama2-7b'\n",
    "dataset_name = 'pile_uncopyrighted_parquet_test'\n",
    "dataset_subset = [0]\n",
    "ctx_len = 1024\n",
    "batch_size = 2\n",
    "model_checkpoints = [\n",
    "    \"main\"\n",
    "]\n",
    "\n",
    "# Generate 12 uniformly distributed layer indices from 0 to 31\n",
    "num_layers = 32  \n",
    "layer_idx_list = [0] + np.linspace(1, num_layers-1, num_layers//2, dtype=int).tolist()\n",
    "\n",
    "# Convert dataset_subset list to a string format\n",
    "dataset_subset_str = ' '.join(map(str, dataset_subset))\n",
    "dataset_subset_str_bar = '_'.join(map(str, dataset_subset))\n",
    "\n",
    "def create_config(model_checkpoint, session_name):\n",
    "    config = {\n",
    "        \"session_name\": session_name,\n",
    "        \"session_path\": f\"/home/gridsan/jsong/physics_dl_shared/ML_JY/ID_scaling/Data/{session_name}\",\n",
    "        \"model_config\": {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_checkpoint\": model_checkpoint,\n",
    "            \"use_accelerator\": False,\n",
    "            \"module_name_mapping\": {\n",
    "                \"mlp\": \"model.layers.{layer}.mlp\",\n",
    "                \"attn\": \"model.layers.{layer}.self_attn\",\n",
    "                \"block\": \"model.layers.{layer}\",\n",
    "                \"emb\": \"model.embed_tokens\",\n",
    "                \"unemb\": \"model.norm\",\n",
    "            }\n",
    "        },\n",
    "        \"dataset_config\": {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"dataset_subset\": dataset_subset,\n",
    "            \"max_dataset_size\": 50000,\n",
    "            \"filter_and_chunk_config\": {\n",
    "                \"min_chunks_from_a_document\": 5,\n",
    "                \"max_chunks_from_a_document\": 5\n",
    "            }\n",
    "        },\n",
    "        \"ctx_len\": ctx_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"cacheing_config\": {\n",
    "            \"layer_idx_list\": layer_idx_list,  # Using the uniformly distributed layer indices\n",
    "            \"module_inblock_keys\": ['block'],\n",
    "            \"module_outblock_keys\": ['unemb'],\n",
    "            \"save_fp\": \"torch.float16\",\n",
    "            \"save_cache_tensors\": True,\n",
    "            \"save_mean_tensors\": True,\n",
    "            \"save_IDs\": True,\n",
    "            \"save_IDs_list\": ['mle', 'mind_ml', 'twoNN_f10'],\n",
    "        },\n",
    "        'multiprocessing': True,\n",
    "        'multiprocessing_num_cpus': 25,\n",
    "        'verbose': True\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def submit_job(model_checkpoint):\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    session_name = f\"{current_time}_{model_name}_{model_checkpoint}_{dataset_name}_{dataset_subset_str_bar}\"\n",
    "    print(f\"Submitting job for {session_name}\")\n",
    "\n",
    "    # Create config\n",
    "    config = create_config(model_checkpoint, session_name)\n",
    "    \n",
    "    # Save config to a YAML file\n",
    "    config_filename = f\"Configs/config_{model_name}_{model_checkpoint}.yaml\"\n",
    "    with open(config_filename, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "    # Create SLURM script\n",
    "    sbatch_script = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={model_name}_{model_checkpoint}\n",
    "#SBATCH --time=72:10:00\n",
    "#SBATCH --output=LOG/%x-%j.out\n",
    "#SBATCH --error=LOG/%x-%j.err\n",
    "#SBATCH -N 1\n",
    "#SBATCH -c 40\n",
    "#SBATCH --gres=gpu:volta:1\n",
    "#SBATCH --mem-per-cpu=7G\n",
    "\n",
    "echo \"Model name\" : {model_name}\n",
    "echo \"Model checkpoint: {model_checkpoint}\"\n",
    "echo \"CPUs per task: $SLURM_CPUS_PER_TASK\"\n",
    "\n",
    "module load anaconda/2023a-pytorch\n",
    "python Run_experiment.py --config_path {config_filename}\n",
    "\"\"\"\n",
    "\n",
    "    script_filename = f\"LLSubScripts/submit_job_{model_name}_{model_checkpoint}.sbatch\"\n",
    "    with open(script_filename, \"w\") as f:\n",
    "        f.write(sbatch_script)\n",
    "\n",
    "    # Submit the job\n",
    "    subprocess.run([\"sbatch\", script_filename])\n",
    "\n",
    "# Submit a job for each model checkpoint\n",
    "for checkpoint in model_checkpoints:\n",
    "    submit_job(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc9fc7c8-96c5-4a6d-a365-fb7a534c985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting job for 20240810_174822_olmo-7b_main_pile_uncopyrighted_parquet_test_0\n",
      "Submitted batch job 26774399\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameters\n",
    "model_name = 'olmo-7b'\n",
    "dataset_name = 'pile_uncopyrighted_parquet_test'\n",
    "dataset_subset = [0]\n",
    "ctx_len = 1024  # As requested\n",
    "batch_size = 2  # As requested\n",
    "model_checkpoints = [\"main\"]\n",
    "\n",
    "# Generate layer indices\n",
    "num_layers = 32  # OLMo 7B has 32 layers\n",
    "layer_idx_list = [0] + np.linspace(1, num_layers-1, num_layers//2, dtype=int).tolist()\n",
    "\n",
    "# Convert dataset_subset list to a string format\n",
    "dataset_subset_str = ' '.join(map(str, dataset_subset))\n",
    "dataset_subset_str_bar = '_'.join(map(str, dataset_subset))\n",
    "\n",
    "def create_config(model_checkpoint, session_name):\n",
    "    config = {\n",
    "        \"session_name\": session_name,\n",
    "        \"session_path\": f\"/home/gridsan/jsong/physics_dl_shared/ML_JY/ID_scaling/Data/{session_name}\",\n",
    "        \"model_config\": {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_checkpoint\": model_checkpoint,\n",
    "            \"use_accelerator\": False,\n",
    "            \"module_name_mapping\": {\n",
    "                \"mlp\": \"model.transformer.blocks.{layer}.ff_out\",\n",
    "                \"attn\": \"model.transformer.blocks.{layer}.att_out\",\n",
    "                \"block\": \"model.transformer.blocks.{layer}\",\n",
    "                \"emb\": \"model.transformer.wte\",\n",
    "                \"unemb\": \"model.transformer.ln_f\",\n",
    "            }\n",
    "        },\n",
    "        \"dataset_config\": {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"dataset_subset\": dataset_subset,\n",
    "            \"max_dataset_size\": 50000,\n",
    "            \"filter_and_chunk_config\": {\n",
    "                \"min_chunks_from_a_document\": 5,\n",
    "                \"max_chunks_from_a_document\": 5\n",
    "            }\n",
    "        },\n",
    "        \"ctx_len\": ctx_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"cacheing_config\": {\n",
    "            \"layer_idx_list\": layer_idx_list,\n",
    "            \"module_inblock_keys\": ['block'],\n",
    "            \"module_outblock_keys\": ['unemb'],\n",
    "            \"save_fp\": \"torch.float16\",\n",
    "            \"save_cache_tensors\": True,\n",
    "            \"save_mean_tensors\": True,\n",
    "            \"save_IDs\": True,\n",
    "            \"save_IDs_list\": ['mle', 'mind_ml', 'twoNN_f10'],\n",
    "        },\n",
    "        'multiprocessing': True,\n",
    "        'multiprocessing_num_cpus': 25,\n",
    "        'verbose': True\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def submit_job(model_checkpoint):\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    session_name = f\"{current_time}_{model_name}_{model_checkpoint}_{dataset_name}_{dataset_subset_str_bar}\"\n",
    "    print(f\"Submitting job for {session_name}\")\n",
    "\n",
    "    # Create config\n",
    "    config = create_config(model_checkpoint, session_name)\n",
    "\n",
    "    # Save config to a YAML file\n",
    "    config_filename = f\"Configs/config_{model_name}_{model_checkpoint}.yaml\"\n",
    "    with open(config_filename, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "    # Create SLURM script\n",
    "    sbatch_script = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={model_name}_{model_checkpoint}\n",
    "#SBATCH --time=72:10:00\n",
    "#SBATCH --output=LOG/%x-%j.out\n",
    "#SBATCH --error=LOG/%x-%j.err\n",
    "#SBATCH -N 1\n",
    "#SBATCH -c 40\n",
    "#SBATCH --gres=gpu:volta:1\n",
    "#SBATCH --mem-per-cpu=7G\n",
    "echo \"Model name: {model_name}\"\n",
    "echo \"Model checkpoint: {model_checkpoint}\"\n",
    "echo \"CPUs per task: $SLURM_CPUS_PER_TASK\"\n",
    "module load anaconda/2023a-pytorch\n",
    "python Run_experiment.py --config_path {config_filename}\n",
    "\"\"\"\n",
    "\n",
    "    script_filename = f\"LLSubScripts/submit_job_{model_name}_{model_checkpoint}.sbatch\"\n",
    "    with open(script_filename, \"w\") as f:\n",
    "        f.write(sbatch_script)\n",
    "\n",
    "    # Submit the job\n",
    "    subprocess.run([\"sbatch\", script_filename])\n",
    "\n",
    "# Submit a job for each model checkpoint\n",
    "for checkpoint in model_checkpoints:\n",
    "    submit_job(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24c370da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting job for 20240810_020645_mistral-7b_main_pile_uncopyrighted_parquet_test_0\n",
      "Submitted batch job 26768690\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameters\n",
    "model_name = 'mistral-7b'\n",
    "dataset_name = 'pile_uncopyrighted_parquet_test'\n",
    "dataset_subset = [0]\n",
    "ctx_len = 1024  # Mistral supports longer contexts, but we'll keep it as requested\n",
    "batch_size = 2  # As requested\n",
    "model_checkpoints = [\"main\"]\n",
    "\n",
    "# Generate layer indices\n",
    "num_layers = 32  # Mistral 7B has 32 layers\n",
    "layer_idx_list = [0] + np.linspace(1, num_layers-1, num_layers//2, dtype=int).tolist()\n",
    "\n",
    "# Convert dataset_subset list to a string format\n",
    "dataset_subset_str = ' '.join(map(str, dataset_subset))\n",
    "dataset_subset_str_bar = '_'.join(map(str, dataset_subset))\n",
    "\n",
    "def create_config(model_checkpoint, session_name):\n",
    "    config = {\n",
    "        \"session_name\": session_name,\n",
    "        \"session_path\": f\"/home/gridsan/jsong/physics_dl_shared/ML_JY/ID_scaling/Data/{session_name}\",\n",
    "        \"model_config\": {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_checkpoint\": model_checkpoint,\n",
    "            \"use_accelerator\": False,\n",
    "            \"module_name_mapping\": {\n",
    "                \"mlp\": \"model.layers.{layer}.feed_forward\",\n",
    "                \"attn\": \"model.layers.{layer}.self_attn\",\n",
    "                \"block\": \"model.layers.{layer}\",\n",
    "                \"emb\": \"model.embed_tokens\",\n",
    "                \"unemb\": \"model.norm\",\n",
    "                \"input_layernorm\": \"model.layers.{layer}.input_layernorm\",\n",
    "                \"post_attention_layernorm\": \"model.layers.{layer}.post_attention_layernorm\"\n",
    "            }\n",
    "        },\n",
    "        \"dataset_config\": {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"dataset_subset\": dataset_subset,\n",
    "            \"max_dataset_size\": 50000,\n",
    "            \"filter_and_chunk_config\": {\n",
    "                \"min_chunks_from_a_document\": 5,\n",
    "                \"max_chunks_from_a_document\": 5\n",
    "            }\n",
    "        },\n",
    "        \"ctx_len\": ctx_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"cacheing_config\": {\n",
    "            \"layer_idx_list\": layer_idx_list,\n",
    "            \"module_inblock_keys\": ['block'],\n",
    "            \"module_outblock_keys\": ['unemb'],\n",
    "            \"save_fp\": \"torch.float16\",\n",
    "            \"save_cache_tensors\": True,\n",
    "            \"save_mean_tensors\": True,\n",
    "            \"save_IDs\": True,\n",
    "            \"save_IDs_list\": ['mle', 'mind_ml', 'twoNN_f10'],\n",
    "        },\n",
    "        'multiprocessing': True,\n",
    "        'multiprocessing_num_cpus': 25,\n",
    "        'verbose': True\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def submit_job(model_checkpoint):\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    session_name = f\"{current_time}_{model_name}_{model_checkpoint}_{dataset_name}_{dataset_subset_str_bar}\"\n",
    "    print(f\"Submitting job for {session_name}\")\n",
    "\n",
    "    # Create config\n",
    "    config = create_config(model_checkpoint, session_name)\n",
    "\n",
    "    # Save config to a YAML file\n",
    "    config_filename = f\"Configs/config_{model_name}_{model_checkpoint}.yaml\"\n",
    "    with open(config_filename, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "    # Create SLURM script\n",
    "    sbatch_script = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={model_name}_{model_checkpoint}\n",
    "#SBATCH --time=72:10:00\n",
    "#SBATCH --output=LOG/%x-%j.out\n",
    "#SBATCH --error=LOG/%x-%j.err\n",
    "#SBATCH -N 1\n",
    "#SBATCH -c 32\n",
    "#SBATCH --gres=gpu:volta:1\n",
    "#SBATCH --mem-per-cpu=7G\n",
    "echo \"Model name: {model_name}\"\n",
    "echo \"Model checkpoint: {model_checkpoint}\"\n",
    "echo \"CPUs per task: $SLURM_CPUS_PER_TASK\"\n",
    "module load anaconda/2023a-pytorch\n",
    "python Run_experiment.py --config_path {config_filename}\n",
    "\"\"\"\n",
    "\n",
    "    script_filename = f\"LLSubScripts/submit_job_{model_name}_{model_checkpoint}.sbatch\"\n",
    "    with open(script_filename, \"w\") as f:\n",
    "        f.write(sbatch_script)\n",
    "\n",
    "    # Submit the job\n",
    "    subprocess.run([\"sbatch\", script_filename])\n",
    "\n",
    "# Submit a job for each model checkpoint\n",
    "for checkpoint in model_checkpoints:\n",
    "    submit_job(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ec0fcda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting job for 20240810_180311_pythia-6.9b-deduped_main_pile_uncopyrighted_parquet_test_0\n",
      "Submitted batch job 26774557\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameters\n",
    "model_name = 'pythia-6.9b-deduped'\n",
    "dataset_name = 'pile_uncopyrighted_parquet_test'\n",
    "dataset_subset = [0]\n",
    "ctx_len = 1024  # Adjust if needed for Pythia\n",
    "batch_size = 2  # As requested\n",
    "model_checkpoints = [\"main\"]\n",
    "\n",
    "# Generate layer indices\n",
    "num_layers = 32  # Pythia-6.9B has 32 layers\n",
    "layer_idx_list = [0] + np.linspace(1, num_layers-1, num_layers//2, dtype=int).tolist()\n",
    "\n",
    "# Convert dataset_subset list to a string format\n",
    "dataset_subset_str = ' '.join(map(str, dataset_subset))\n",
    "dataset_subset_str_bar = '_'.join(map(str, dataset_subset))\n",
    "\n",
    "def create_config(model_checkpoint, session_name):\n",
    "    config = {\n",
    "        \"session_name\": session_name,\n",
    "        \"session_path\": f\"/home/gridsan/jsong/physics_dl_shared/ML_JY/ID_scaling/Data/{session_name}\",\n",
    "        \"model_config\": {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_checkpoint\": model_checkpoint,\n",
    "            \"use_accelerator\": False,\n",
    "            \"module_name_mapping\": {\n",
    "                \"mlp\": \"gpt_neox.layers.{layer}.mlp\",\n",
    "                \"attn\": \"gpt_neox.layers.{layer}.attention\",\n",
    "                \"block\": \"gpt_neox.layers.{layer}\",\n",
    "                \"emb\": \"gpt_neox.embed_in\",\n",
    "                \"unemb\": \"gpt_neox.final_layer_norm\",\n",
    "                \"input_layernorm\": \"gpt_neox.layers.{layer}.input_layernorm\",\n",
    "                \"post_attention_layernorm\": \"gpt_neox.layers.{layer}.post_attention_layernorm\"\n",
    "            }\n",
    "        },\n",
    "        \"dataset_config\": {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"dataset_subset\": dataset_subset,\n",
    "            \"max_dataset_size\": 50000,\n",
    "            \"filter_and_chunk_config\": {\n",
    "                \"min_chunks_from_a_document\": 5,\n",
    "                \"max_chunks_from_a_document\": 5\n",
    "            }\n",
    "        },\n",
    "        \"ctx_len\": ctx_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"cacheing_config\": {\n",
    "            \"layer_idx_list\": layer_idx_list,\n",
    "            \"module_inblock_keys\": ['block'],\n",
    "            \"module_outblock_keys\": ['unemb'],\n",
    "            \"save_fp\": \"torch.float16\",\n",
    "            \"save_cache_tensors\": True,\n",
    "            \"save_mean_tensors\": True,\n",
    "            \"save_IDs\": True,\n",
    "            \"save_IDs_list\": ['mle', 'mind_ml', 'twoNN_f10'],\n",
    "        },\n",
    "        'multiprocessing': True,\n",
    "        'multiprocessing_num_cpus': 25,\n",
    "        'verbose': True\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def submit_job(model_checkpoint):\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    session_name = f\"{current_time}_{model_name}_{model_checkpoint}_{dataset_name}_{dataset_subset_str_bar}\"\n",
    "    print(f\"Submitting job for {session_name}\")\n",
    "\n",
    "    # Create config\n",
    "    config = create_config(model_checkpoint, session_name)\n",
    "\n",
    "    # Save config to a YAML file\n",
    "    config_filename = f\"Configs/config_{model_name}_{model_checkpoint}.yaml\"\n",
    "    with open(config_filename, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "    # Create SLURM script\n",
    "    sbatch_script = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={model_name}_{model_checkpoint}\n",
    "#SBATCH --time=72:10:00\n",
    "#SBATCH --output=LOG/%x-%j.out\n",
    "#SBATCH --error=LOG/%x-%j.err\n",
    "#SBATCH -N 1\n",
    "#SBATCH -c 32\n",
    "#SBATCH --gres=gpu:volta:1\n",
    "#SBATCH --mem-per-cpu=7G\n",
    "\n",
    "echo \"Model name: {model_name}\"\n",
    "echo \"Model checkpoint: {model_checkpoint}\"\n",
    "echo \"CPUs per task: $SLURM_CPUS_PER_TASK\"\n",
    "\n",
    "module load anaconda/2023a-pytorch\n",
    "python Run_experiment.py --config_path {config_filename}\n",
    "\"\"\"\n",
    "\n",
    "    script_filename = f\"LLSubScripts/submit_job_{model_name}_{model_checkpoint}.sbatch\"\n",
    "    with open(script_filename, \"w\") as f:\n",
    "        f.write(sbatch_script)\n",
    "\n",
    "    # Submit the job\n",
    "    subprocess.run([\"sbatch\", script_filename])\n",
    "\n",
    "# Submit a job for each model checkpoint\n",
    "for checkpoint in model_checkpoints:\n",
    "    submit_job(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3de70775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting job for 20240810_171854_gemma-7b_main_pile_uncopyrighted_parquet_test_0\n",
      "Submitted batch job 26774205\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import numpy as np\n",
    "\n",
    "# Define the hyperparameters\n",
    "model_name = 'gemma-7b'  # or 'gemma-2b'\n",
    "dataset_name = 'pile_uncopyrighted_parquet_test'\n",
    "dataset_subset = [0]\n",
    "ctx_len = 1024  # Gemma supports up to 8192\n",
    "batch_size = 1  # Reduced batch size due to model size\n",
    "model_checkpoints = [\"main\"]\n",
    "\n",
    "# Generate layer indices\n",
    "num_layers = 28\n",
    "layer_idx_list = [0] + np.linspace(1, num_layers-1, num_layers//4+1, dtype=int).tolist()\n",
    "\n",
    "# Convert dataset_subset list to a string format\n",
    "dataset_subset_str = ' '.join(map(str, dataset_subset))\n",
    "dataset_subset_str_bar = '_'.join(map(str, dataset_subset))\n",
    "\n",
    "def create_config(model_checkpoint, session_name):\n",
    "    config = {\n",
    "        \"session_name\": session_name,\n",
    "        \"session_path\": f\"/home/gridsan/jsong/physics_dl_shared/ML_JY/ID_scaling/Data/{session_name}\",\n",
    "        \"model_config\": {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_checkpoint\": model_checkpoint,\n",
    "            \"use_accelerator\": True,\n",
    "            \"module_name_mapping\": {\n",
    "                \"mlp\": \"model.layers.{layer}.mlp\",\n",
    "                \"attn\": \"model.layers.{layer}.attention\",\n",
    "                \"block\": \"model.layers.{layer}\",\n",
    "                \"emb\": \"model.embed_tokens\",\n",
    "                \"unemb\": \"model.norm\",\n",
    "                \"rotary_emb\": \"model.layers.{layer}.attention.rotary_emb\",\n",
    "                \"input_layernorm\": \"model.layers.{layer}.input_layernorm\",\n",
    "                \"post_attention_layernorm\": \"model.layers.{layer}.post_attention_layernorm\"\n",
    "            }\n",
    "        },\n",
    "        \"dataset_config\": {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"dataset_subset\": dataset_subset,\n",
    "            \"max_dataset_size\": 50000,\n",
    "            \"filter_and_chunk_config\": {\n",
    "                \"min_chunks_from_a_document\": 5,\n",
    "                \"max_chunks_from_a_document\": 5\n",
    "            }\n",
    "        },\n",
    "        \"ctx_len\": ctx_len,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"cacheing_config\": {\n",
    "            \"layer_idx_list\": layer_idx_list,\n",
    "            \"module_inblock_keys\": ['block'],\n",
    "            \"module_outblock_keys\": ['unemb'],\n",
    "            \"save_fp\": \"torch.float16\",\n",
    "            \"save_cache_tensors\": True,\n",
    "            \"save_mean_tensors\": True,\n",
    "            \"save_IDs\": True,\n",
    "            \"save_IDs_list\": ['mle', 'mind_ml', 'twoNN_f10'],\n",
    "        },\n",
    "        'multiprocessing': True,\n",
    "        'multiprocessing_num_cpus': 25,  # Adjusted for potential memory constraints\n",
    "        'verbose': True\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def submit_job(model_checkpoint):\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    session_name = f\"{current_time}_{model_name}_{model_checkpoint}_{dataset_name}_{dataset_subset_str_bar}\"\n",
    "    print(f\"Submitting job for {session_name}\")\n",
    "\n",
    "    # Create config\n",
    "    config = create_config(model_checkpoint, session_name)\n",
    "\n",
    "    # Save config to a YAML file\n",
    "    config_filename = f\"Configs/config_{model_name}_{model_checkpoint}.yaml\"\n",
    "    with open(config_filename, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "    # Create SLURM script\n",
    "    sbatch_script = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={model_name}_{model_checkpoint}\n",
    "#SBATCH --time=72:10:00\n",
    "#SBATCH --output=LOG/%x-%j.out\n",
    "#SBATCH --error=LOG/%x-%j.err\n",
    "#SBATCH -N 1\n",
    "#SBATCH -c 40\n",
    "#SBATCH --gres=gpu:volta:1\n",
    "#SBATCH --mem-per-cpu=7G\n",
    "echo \"Model name: {model_name}\"\n",
    "echo \"Model checkpoint: {model_checkpoint}\"\n",
    "echo \"CPUs per task: $SLURM_CPUS_PER_TASK\"\n",
    "module load anaconda/2023a-pytorch\n",
    "python Run_experiment.py --config_path {config_filename}\n",
    "\"\"\"\n",
    "\n",
    "    script_filename = f\"LLSubScripts/submit_job_{model_name}_{model_checkpoint}.sbatch\"\n",
    "    with open(script_filename, \"w\") as f:\n",
    "        f.write(sbatch_script)\n",
    "\n",
    "    # Submit the job\n",
    "    subprocess.run([\"sbatch\", script_filename])\n",
    "\n",
    "# Submit a job for each model checkpoint\n",
    "for checkpoint in model_checkpoints:\n",
    "    submit_job(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bf0145",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
